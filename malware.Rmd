---
title: "Classification of Malware using static and dynamic features"
subtitle: "HarvardX Data Science Capstone Project"
author: "Raphael Kummer"
date: "`r format(Sys.Date())`"
output: 
  pdf_document:
    df_print: kable
    toc: yes
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, progress = TRUE, verbose = TRUE)

if(!require(pacman)) install.packages("pacman")
library(pacman)
p_load(ggplot2, benchmarkme, rmarkdown, lubridate, scales, parallel, stringr, kableExtra, tidyverse, caret, readr, httr, e1071, class, ranger, rpart, xgboost, nnet, glmnet, GGally, reshape2, plyr, scales)
```
\newpage
# Malware
## Introduction
Malicious software (Malware) is a software used to harm, damage, access or alter computer systems or networks. There are several types of Malware, including viruses, worms, trojan horses, ransomware and spyware. Most malware access computer systems by internet and emails. They can hide in legitimate applications or are hidden in the system.
Static analyses can be effective, but also easily evaded through obfuscation and altering the malware so much, that it is no longer recognized. Therefore additional dynamic features are used to classify malwares.

### Dataset
The dataset *Malware static and dynamic features VxHeaven and Virus Total Data Set* [2] from the UCI Machine Learning Repository contains three csv. 

* staDynBenignLab.csv: 1086 features extracted from 595 files on MS Windows 7 and 8, obtained Program Files directory.
* staDynVxHeaven2698Lab.csv: 1087 features extracted from 2698 files of VxHeaven dataset.
* staDynVt2955Lab.csv: 1087 features extracted from 2955 provided by Virus Total in 2018.

The staDynBenignLab has all the features of typical non-threatening programs wheare as staDynVxHeaven2698Lab and staDynVt2955Lab are extracted features of malware programs from the databases of VxHeaven and Virus Total. 
The dataset provides static features like ASM, compiler version, operating system version, Hex dump and PE header [3] (portable executable) and dynamic features extracted from a cuckoo sandbox [4].
The dataset only contains windows specific programs and malware.
The dataset is downloaded from the UCI Machine Learning Repository Dataset and the containing csv files are extracted. The data in each of the csv must be labeled first, describing the class of program identified (e.g. class 0 for benign, class 1 for static malware features and class 2 for dynamic malware features).

## Goal
The dataset is used to explore and gain insight on how normal programs and malware differ in static and dynamic structure. Several machine learning algorithms to classify an unseen program as benign or malware (static or dynamic) are developed and compared.
Overall, the goal of this project is to improve our understanding of how malware differs from normal programs, and to develop effective techniques for automatically identifying and classifying malware based on its static and dynamic features.

## Summary
We are able to correctly identify between benign programs and Malware (static or dynamic) with > 95% accuracy and also good F1 scores with the Random Forest model.

\newpage
# Analysis
Disclaimer: Not all R code will be shown in this report. For the full code listing, view the R-File on github.

## Download
The zipped dataset is downloaded from the UCI archive and unzipped. 
  
```{r include=FALSE, echo=FALSE, warning=FALSE}
# download dataset from UCI archive https://archive.ics.uci.edu/ml/datasets/Malware+static+and+dynamic+features+VxHeaven+and+Virus+Total#
download_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00541/datasets_DetectingMalwareUsingAHybridApproach.zip'
download.file(download_url, 'datasets.zip', mode='wb')

# unzip
unzip_result <- unzip("datasets.zip", overwrite = TRUE, exdir="data")
```
  
This will extract the following files: `r list.files("data")`

```{r include=FALSE, echo=FALSE, warning=FALSE}
 # load these csv
benign <- read_csv("data/staDynBenignLab.csv")
vxheaven <- read_csv("data/staDynVxHeaven2698Lab.csv")
vt <- read_csv("data/staDynVt2955Lab.csv")
```
 
Where 

* *staDynBenignLab.csv*has `r { ncol(benign) }` features extracted from `r { nrow(benign) }` files on MS Windows 7 and 8, obtained Program Files directory
* *staDynVxHeaven2698Lab.csv* has `r { ncol(vxheaven) }` features extracted from `r { nrow(vxheaven) }` files of VxHeaven dataset.
* *staDynVt2955Lab.csv* has `r { ncol(vt) }` features extracted from `r { nrow(vt) }` provided by Virus Total in 2018.

## Preprocessing
Add classes to each dataset, depending on the origin of their data:
      
* 0: Normal *benign* programs 
* 1: Malware with static features extracted from the *VxHeaven (vxheaven)* dataset
* 2: Malware with dynamic features extracted from the *Virus Total (vt)* dataset

```{r echo=FALSE}
# classify each dataset
benign$class <- 0
vxheaven$class <- 1
vt$class <- 2
```
  
## Cleanup
The datasets have 

* staDynBenignLab: `r sum(is.na(benign))`, 
* staDynVxHeaven2698Lab: `r sum(is.na(vxheaven))` and 
* staDynVt2955Lab: `r sum(is.na(vt))` 

missing values.

Check for features that not present in all three datasets, remove unique columns and combine all in one large dataset. 
```{r echo=FALSE}   
# list columns of all datasets
cols_benign <- colnames(benign)
cols_vxheaven <- colnames(vxheaven)
cols_vt <- colnames(vt)

# Find common column names
common_cols <- intersect(intersect(cols_benign, cols_vxheaven), cols_vt)

# Find different column names
different_cols <- setdiff(union(union(cols_benign, cols_vxheaven), cols_vt), common_cols)

# remove unique columns
benign <- benign[, common_cols]
vxheaven <- vxheaven[, common_cols]
vt <- vt[, common_cols]
# now each dataset has 1085 features

# create one large dataset 
data <- rbind(benign, vxheaven, vt)
```
These columns are not present in all datasets:

> `r different_cols`

So they are removed and the resulting dataset we now work with has `r { ncol(benign) }` features for each of the `r { nrow(benign) }` different programs.

## Variance
Check feature variance for features without any relevant information, find all these features where the variation is 0 and remove them from the dataset.

```{r echo=TRUE, warning=FALSE}
# Calculate the variance of each variable in the dataset
variances <- apply(data, 2, var)

# Find which variances are equal to 0
zero_variances <- which(variances == 0)

# Exclude any variables with 0 variance
nonzero_variances <- variances[variances > 0]
```

`r { length(zero_variances)}` found to have a variance of 0. E.g. all variables of "count_file_renamed" are 0 so there is no benefit for the analysis or training of models. These are removed from the dataset and the remaining variances are plotted here:

```{r include=FALSE, echo=TRUE}
# Create a bar plot of the variances
barplot(nonzero_variances, main="Feature Variance", las=2, log="y", xlab="", ylab="variance log10")
```

When inspecting the variable names it is clear, that variables with the same purpose are grouped together. 

Lets unpack the different parts of this plot further. For this extract all the column names of the nonzero variances.

```{r include=FALSE}
# create a dataset with only relevant variables
nonzero_cols <- colnames(data)[variances != 0]
data_v2 <- data[, nonzero_cols]
```

> ``r nonzero_cols[1:27]``

In the first section there are datatypes (UINT, LONG, BOOL, WORD) and some static file characteristics (handle,dll,.data,.text). Then follows a big block of ASM operations (add, mov, or, mul, nop, ...) 

> ``r nonzero_cols[28:103]``

also with significant variance compared to other groups, like the function calls to the Windows operating system API.

> ``r nonzero_cols[104:191]``

These show a low variance. Some of the highest variances are found in the last section.

> ``r nonzero_cols[192:245]``

These describe program features like the operating systems characteristics, compiler used and how the program was set up (like heap and stack usage, memory allocation, headers and dlls, ...).
Overall the ASM functions and mixed section where a mix of OS characteristics, compiler features and program usage are listed, show the most variance.

## Feature Clusters
Cluster some features that are most likely to be compared against each other like data types (e.g. uint,int,char,word,byte,int...) or File specific characteristics or assembly functions. Some I could not figure out the meaning behind it (like ent_whole_file or ent_var) so these are left out. 

### Datatypes
C is a strongly typed language, which means that every variable must have a specific data type assigned to it. The size of these datatypes can be different depending on the platform and/or compiler. Commonly used dattypes are BOOL (true, false), integers (like UINT, int, LONG, ...) and natural numbers (float, double). 
There are several datatype features recorded in the dataset, lets make a group of datatypes.

```{r include=FALSE, echo=TRUE, warning=FALSE}
# list datatypes + class
types <- c('UINT', 'LONG', 'BOOL', 'WORD','byte','word','char','int','class')

# subset of these types 
types_subset <- subset(data_v2, select=types)
# give the classes some better names
types_subset$class <- factor(types_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))

# boxplot for the type "int"
ggplot(types_subset, aes(x = class, y = int, fill=factor(class))) +
  geom_boxplot(alpha = 0.8) + 
  scale_fill_manual(values = c("lightskyblue", "orchid", "violetred"), 
                    labels = c("benign", "Malware static", "Malware dynamic")) +
  scale_y_log10(limits = c(1, max(types_subset$int))) +
  labs(x = "Class", y = "log(int)") 
```

The static malware has generally lower number of datatypes "int" than the other classes. But benign and dynamic Malware are quite similar, although bening is a bit denser packet around the mean.
There are some types that have data only in one class, e.g. char.

```{r include=FALSE, echo=TRUE, warning=FALSE}
ggplot(types_subset, aes(x = class, y = char, fill=factor(class))) +
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = c("lightskyblue", "orchid", "violetred"), 
                    labels = c("benign", "Malware static", "Malware dynamic")) +
  scale_y_log10(limits = c(1, max(types_subset$char))) +
  labs(x = "Class", y = "log(char)")  
```

Here a plot for all datatypes. Only byte and int have variables in all classes. WORD and word are probably the same and can be compared but there is no word for the benign class.

```{r include=FALSE, echo=TRUE, warning=FALSE}
types_subset.m <- melt(types_subset, id.var = "class")

ggplot(data = types_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = c("lightskyblue", "orchid", "violetred"), 
                    labels = c("benign", "Malware static", "Malware dynamic")) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA))

#  scale_y_log10(limits = c(1, max(types_subset.m$value)), expand = c(0, 0))
```

In a scatterplot Matrix it shows that some features are heavy correlated, like UINT correlates with LONG, BOOL and WORD but not with byte, word, char and int.

```{r include=FALSE, echo=TRUE, warning=FALSE}
ggpairs(types_subset)

types_subset_matrix = data.matrix(types_subset)
heatmap(types_subset_matrix,
        Rowv=NA,
        Colv=NA,
        col=colorRampPalette(c("lightgrey","lightblue"))(100),
        scale="none")

##types_subset_matrix = data.matrix(types_subset)
#heatmap(types_subset.m,
#        Rowv=NA,
#        Colv=NA,
#        col=colorRampPalette(c("lightgrey","lightblue"))(100),
#        scale="none")
        

types_subset.m <- melt(types_subset, id.var = "class")
types_subset.m <- ddply(types_subset.m, .(variable), transform, rescale=rescale(value))
ggplot(types_subset.m, aes(variable, class)) +
  geom_tile(aes(fill = rescale), colour = "grey") +
  scale_fill_gradient(low = "lightblue", high = "steelblue")
```

### File/exe
File characteristics like size of number of sections or runtime fingerprints like loaded dll's or stack/heap sizes.

```{r include=FALSE, echo=TRUE, warning=FALSE}
# column with file characteristics
file_1 <- c('filesize','number_of_rva_and_sizes','size_code','SizeOfHeaders','section_alignment','file_alignment','size_of_headers','subsystem','dll_characteristics','AddressOfEntryPoint','class')

# create a second one for two smaller plots
file_2 <- c('size_of_stack_reserve','size_of_stack_commit','size_of_heap_reserve','size_of_heap_commit','image_base','Size_image','BaseOfCode','number_of_rva_and_sizes.1','number_of_IAT_entires.1','files_operations','count_file_opened','count_dll_loaded','class')

# subset of these types 
file_1_subset <- subset(data_v2, select=file_1)
file_2_subset <- subset(data_v2, select=file_2)

# give the classes some better names
file_1_subset$class <- factor(file_1_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))
file_2_subset$class <- factor(file_2_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))

file_1_subset.m <- melt(file_1_subset, id.var = "class")
file_2_subset.m <- melt(file_2_subset, id.var = "class")

ggplot(data = file_1_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = c("lightskyblue", "orchid", "violetred"), 
                    labels = c("benign", "Malware static", "Malware dynamic")) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA))

ggplot(data = file_2_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = c("lightskyblue", "orchid", "violetred"), 
                    labels = c("benign", "Malware static", "Malware dynamic")) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA))
```

Some do not have characteristic data for all three classes, eg. .rdata, .data, loader_flags, count_mutex, count_file_read, count_file_written, count_file_exists, count_file_deleted, count_file_copied, count_regkey_written and count_regkey_deleted. These are exclusively available for static and dynamic malware. Wile most  are quite similar in their distribution, loader_flags is very different between the two classes. 

```{r include=FALSE, echo=TRUE, warning=FALSE}
file2 <- c('loader_flags','count_mutex','count_file_read','count_file_written','count_file_exists','count_file_deleted','count_file_copied','count_regkey_written','count_regkey_deleted','class')

# subset of these types 
file2_subset <- subset(data_v2, select=file2)

# give the classes some better names
file2_subset$class <- factor(file2_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))

file2_subset.m <- melt(file2_subset, id.var = "class")

ggplot(data = file2_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = c("orchid", "violetred"), 
                    labels = c("Malware static", "Malware dynamic")) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA))
```
```{r echo=TRUE, warning=FALSE}
ggpairs(file_1_subset)
ggpairs(file_2_subset)

###############################3
file_1_subset_matrix = data.matrix(file_1_subset)
heatmap(file_1_subset_matrix,
        Rowv=NA,
        Colv=NA,
        col=colorRampPalette(c("lightgrey","lightskyblue"))(100),
        scale="none")

file_2_subset_matrix = data.matrix(file_2_subset)
heatmap(file_2_subset_matrix,
        Rowv=NA,
        Colv=NA,
        col=colorRampPalette(c("lightgrey","lightskyblue"))(100),
        scale="none")


# subset of these types 
file_subset <- subset(data_v2, select=c(file_1,file2))
# give the classes some better names
#file_subset$class <- factor(file_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))
file_subset.m <- melt(file_subset, id.var = "class")

file_subset.m <- ddply(file_subset.m, .(variable), transform, rescale=rescale(value))
ggplot(file_subset.m, aes(variable, class)) +
  geom_tile(aes(fill = rescale), colour = "grey") +
  scale_fill_gradient(low = "white", high = "steelblue")

##################################3
ggparcoord(file_1_subset, 
           columns = 1:ncol(file_1_subset), 
           groupColumn = "class", 
           showPoints = FALSE, 
           alphaLines = 0.3)
```


ggparcoord(file_1_subset, 
           columns = 1:ncol(file_1_subset), 
           groupColumn = "class", 
           showPoints = FALSE, 
           alphaLines = 0.3)
           
ggplot(file_subset.m, aes(variable, class)) +
  geom_tile(aes(fill = rescale), colour = "grey") +
  scale_fill_gradient(low = "white", high = "steelblue")
           
           
           
### Compiler/Linker
```{r echo=TRUE, warning=FALSE}
compiler <- c('characteristics','magic','size_init_data','size_uninit_data','compile_date','major_operating_system_version','minor_operating_system_version','major_image_version','minor_image_version','major_subsystem_version','minor_subsystem_version','major_linker_version','minor_linker_version','class')

# subset of these comiler/linker statistics 
compiler_subset <- subset(data_v2, select=compiler)

# give the classes some better names
compiler_subset$class <- factor(compiler_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))

compiler_subset.m <- melt(compiler_subset, id.var = "class")

ggplot(data = compiler_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = c("lightskyblue", "orchid", "violetred"), 
                    labels = c("benign", "Malware static", "Malware dynamic")) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA))
```

```{r echo=TRUE, warning=FALSE}
ggpairs(compiler_subset)

compiler_subset_matrix = data.matrix(compiler_subset)
heatmap(compiler_subset_matrix,
        Rowv=NA,
        Colv=NA,
        col=colorRampPalette(c("lightgrey","lightskyblue"))(100),
        scale="none")

ggparcoord(compiler_subset, 
           columns = 1:ncol(compiler_subset), 
           groupColumn = "class", 
           showPoints = FALSE, 
           alphaLines = 0.3)


# subset of these types 
compiler_subset.m <- subset(data_v2, select=compiler)
# give the classes some better names
#file_subset$class <- factor(file_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))
compiler_subset.m <- melt(compiler_subset, id.var = "class")

compiler_subset.m <- ddply(compiler_subset.m, .(variable), transform, rescale=rescale(value))
ggplot(compiler_subset.m, aes(variable, class)) +
  geom_tile(aes(fill = rescale), colour = "grey") +
  scale_fill_gradient(low = "white", high = "steelblue")
```

### ASM/Functions
There are way to many assembly functions listed for plotting. so some are selected.

```{r echo=TRUE, warning=FALSE}
#c('dd','db','dw','stdcall','arg','edx','esi','es','fs','ds','ss','gs','cs','ah','al','ax','bh','bl','bx','ch','cl','cx','dh')
#c('dl','dx','eax','ebp','ebx','ecx','edi','esp','add','al.1','bt','call','cdq')
#c('cld','cli','cmc','cmp','const','cwd','daa','db.1','dd.1','dec','dw.1','enp','ends')
#c('faddp','fchs','fdiv','fdivp','fdivr','fild','fistp','fld','fstcw','fstcwimul','fstp','fword','fxch')
#c('imul','in','inc','ins','int.1','jb','je','jg','jge','jl','jmp','jnb','jno','jnz')
#c('jo','jz','lea','loope','mov','movzx','mul','near','neg','not','or','out','outs','pop','popf','proc.1','push','pushf','rcl')
#c('rcr','rdtsc','rep','ret','retn','rol','ror','sal','sar','sbb','scas','setb','setle','setnle','setnz','setz','shl','shld','shr','	sidt','stc','std','sti','stos','sub','test','wait','xchg','xor','nop','wcslen')

asm_1 <- c('dd','db','dw','arg','add','cli','cmc','cmp','imul','inc','int.1','jb','je','jg','jge','jl','class')
asm_2 <- c('jmp','jno','jo','lea','loope','mov','mul','neg','not','or','sub','test','xchg','xor','nop','class')

# subset of these comiler/linker statistics 
asm_1_subset <- subset(data_v2, select=asm_1)
asm_2_subset <- subset(data_v2, select=asm_2)

# give the classes some better names
asm_1_subset$class <- factor(asm_1_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))
asm_2_subset$class <- factor(asm_2_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))

asm_1_subset.m <- melt(asm_1_subset, id.var = "class")
asm_2_subset.m <- melt(asm_2_subset, id.var = "class")

ggplot(data = asm_1_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = c("lightskyblue", "orchid", "violetred"), 
                    labels = c("benign", "Malware static", "Malware dynamic")) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA))

ggplot(data = asm_2_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = c("lightskyblue", "orchid", "violetred"), 
                    labels = c("benign", "Malware static", "Malware dynamic")) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA))
```

There we probably see the most significant difference between the three classes.

```{r echo=TRUE, warning=FALSE}
ggpairs(asm_1_subset)
ggpairs(asm_2_subset)


# subset of these types 
asm_subset <- subset(data_v2, select=c(asm_1, asm_2))
# give the classes some better names
asm_subset$class <- factor(asm_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))
asm_subset.m <- melt(asm_subset, id.var = "class")

asm_subset.m <- ddply(asm_subset.m, .(variable), transform, rescale=rescale(value))
ggplot(asm_subset.m, aes(variable, class)) +
  geom_tile(aes(fill = rescale), colour = "grey") +
  scale_fill_gradient(low = "white", high = "steelblue")
```  

### DLL
Dynamic Link Library (DLL) contains code and data that can be used by several programs at the same time. Common tasks and algorithms can be provided by the OS or programs to give other users defined and tested functions, like kernel32.dll that contains various system functions and resources (IO, memory management, error handling, ...).

```{r echo=TRUE, warning=FALSE}
dlls <- c('dll','dll_characteristics','count_dll_loaded','class')

# subset of these comiler/linker statistics 
dll_subset <- subset(data_v2, select=dlls)

# give the classes some better names
dll_subset$class <- factor(dll_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))

dll_subset.m <- melt(dll_subset, id.var = "class")

ggplot(data = dll_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = c("lightskyblue", "orchid", "violetred"), 
                    labels = c("benign", "Malware static", "Malware dynamic")) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA))



# subset of these types 
asm_subset <- subset(data_v2, select=c(asm_1, asm_2))
# give the classes some better names
asm_subset$class <- factor(asm_subset$class, levels = c(0, 1, 2), labels = c("benign", "Malware static", "Malware dynamic"))
asm_subset.m <- melt(asm_subset, id.var = "class")

dll_subset.m <- ddply(dll_subset.m, .(variable), transform, rescale=rescale(value))
ggplot(dll_subset.m, aes(variable, class)) +
  geom_tile(aes(fill = rescale), colour = "grey") +
  scale_fill_gradient(low = "white", high = "steelblue")
```
Malware generally loads more dll's than legit programs. Maybe due to the fact, that in hiding in a legitimate program it must provide the functionality of the application and some harmful code.
  
\newpage
# Models
We build and test several machine learning approaches to classify unseen programs (according to their static and dynamic features) as benign or possibly malicious, differenciate between identified according to static or dynamic features.
The dataset is split into two parts, one for training and anoter for testing the model. The proportion of the data allocated to the test set is 20% of the complete data, the other 80% are used for the training set.

The goal is to achieve a high accuracy (>90%) with balanced precision and recall (F1) over all classes. 

```{r echo=FALSE, warning=FALSE}
set.seed(42) # because 42 is always the answer

# split data into training and test set
test_index <- createDataPartition(y=data_v2$class, times=1, p=0.2, list=FALSE)
colnames(data_v2) <- make.names(colnames(data_v2))

train_set <- data_v2[-test_index,]
test_set <- data_v2[test_index,]

ml_results <- tibble()
``` 
  
## Guessing
The simplest approach, though not very usefull, would be by simply guessing the classification of a program. It is expected that the accuracy would be 33.3%. 

```{r echo=FALSE, warning=FALSE}
# "Train" the guess model, apply randomly 0,1 or 2
guess_model <- factor(sample(c(0, 1, 2), length(test_set$class), replace=TRUE), levels=c(0,1,2))
guess_acc <- sum(guess_model == test_set$class) / nrow(test_set)
```

```{r include=FALSE, echo=TRUE, warning=FALSE}
# Create the confusion matrix
guess_cm <- confusionMatrix(table(factor(test_set$class), guess_model))

# Get overall accuracy and F1 scores
guess_acc <- guess_cm$overall['Accuracy']
guess_f1_c0 <- guess_cm$byClass[, 'F1'][1]
guess_f1_c1 <- guess_cm$byClass[, 'F1'][2]
guess_f1_c2 <- guess_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Guessing", Accuracy=guess_acc, "F1 Class0"=guess_f1_c0, "F1 Class1"=guess_f1_c1, "F1 Class2"=guess_f1_c2))
```

The achieved accuracy is `r guess_acc`, close to the expected 33.3%.

`r knitr::kable(ml_results)`
  
## Naive Bayes
Naive Bayes is a simple and fast algorithm often used for classifications. The algorithm is based on the Bayes' theorem, that the probability of an event might be related to the event based on knowledge of conditions. 

$P(A|B)=P(B|A)*P(A)/P(B)$

Naive means that the features are mutually independent, so a the probability of one feature does not affect the probability of another feature.
There are several Naive Bayes variants:
  
  * Gaussian Naive Bayes where the features are normally distributed
  * Bernoulli Naive Bayes where the features are binary
  * Multinomial Naive Bayes where frequency of occurences of features is calculated
  
Once the likelihood and prior probabilities have been calculated, Naive Bayes uses Bayes' theorem to compute the probability of each class label for a given set of feature values. The class label with the highest probability is then selected as the predicted label for these set of input data.
The algorithm can be effective for classifications when lots of features are involved, but performance may suffer when features are not independent or unbalanced distributed.
  
```{r echo=TRUE, warning=FALSE}
# Train the Naive Bayes model
nb_model <- naiveBayes(class ~ ., data=train_set)
nb_pred <- predict(nb_model, newdata=test_set)
```

```{r include=FALSE, echo=TRUE, warning=FALSE}
# creater confusion matrix
nb_cm <- confusionMatrix(table(factor(test_set$class), nb_pred))

# Get overall accuracy and F1 scores
nb_acc <- nb_cm$overall['Accuracy']
nb_f1_c0 <- nb_cm$byClass[, 'F1'][1]
nb_f1_c1 <- nb_cm$byClass[, 'F1'][2]
nb_f1_c2 <- nb_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Naive Bayes", Accuracy=nb_acc, "F1 Class0"=nb_f1_c0, "F1 Class1"=nb_f1_c1, "F1 Class2"=nb_f1_c2))
```

The Naive Bayes is not a route to follow further, we get a very low overall accuracy `r nb_acc`, lower than guessing.

`r knitr::kable(ml_results)`

## SVM
Support Vector Machine (SVM) is a supervised machine learning algorithm used for regression or classification tasks. The algorithm tries to fit hyperplanes that separates the data into different classes. In the training finding the hyperplane with the largest margin (distance between the hyperplane and the closest data points from each class).  
In 2-D the hyperplane is basically a line, that separates the classes. Non-binary classification can be done by transforming the input features in higher dimensional space where it can be separated by hyperplanes.
    
SVM are powerfull if the data is clearly separated, but must be used carefully with noisy data or imbalanced classes.

```{r echo=TRUE, warning=FALSE}
# train SVM model
svm_model <- svm(class ~ ., data=train_set)
svm_pred <- factor(round(predict(svm_model, newdata=test_set)), levels=c(0,1,2))
```

```{r include=FALSE, echo=TRUE, warning=FALSE}
# creater confusion matrix
svm_cm <- confusionMatrix(table(factor(test_set$class), svm_pred))

# Get overall accuracy and F1 scores
svm_acc <- svm_cm$overall['Accuracy']
svm_f1_c0 <- svm_cm$byClass[, 'F1'][1]
svm_f1_c1 <- svm_cm$byClass[, 'F1'][2]
svm_f1_c2 <- svm_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="SVM", Accuracy=svm_acc, "F1 Class0"=svm_f1_c0, "F1 Class1"=svm_f1_c1, "F1 Class2"=svm_f1_c2))
```

With `r svm_acc` we are now better than guessing, but the F1 scores are still a bit low.

`r kable(ml_results)`  

    
## KNN
K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for regression or classification tasks. It uses the proximity to grouping to make predictions of an individual point, assuming similar points can be found close to another. 

Small k values can lead to overfitting and the model becomes to sensitive to noise and outliers, whereas a large k may result in underfitting and the model does not capture patterns in the data.

The KNN algorithm will perform best with a small number of features. Therefore we don't expect have high hopes for this approach.
  
```{r echo=TRUE, warning=FALSE}
knn_model <- knn(train_set[, -ncol(train_set)], test_set[, -ncol(test_set)], train_set$class, k = 3)
```

```{r include=FALSE, echo=TRUE, warning=FALSE}
# creater confusion matrix
knn_cm <- confusionMatrix(table(factor(test_set$class), knn_model))

# Get overall accuracy and F1 scores
knn_acc <- knn_cm$overall['Accuracy']
knn_f1_c0 <- knn_cm$byClass[, 'F1'][1]
knn_f1_c1 <- knn_cm$byClass[, 'F1'][2]
knn_f1_c2 <- knn_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="KNN", Accuracy=knn_acc, "F1 Class0"=knn_f1_c0, "F1 Class1"=knn_f1_c1, "F1 Class2"=knn_f1_c2))
```

Accuracy is now very good `r knn_acc`, and also the F1 scores around 0.9.

`r kable(ml_results)`  
    
## Decision Trees
Decision Trees can be used for regression and classification models. Each node in the tree represents a decision on feature which are followed until a leaf node is reached, which represents a class.
These trees are easy to visualize and interpret. But if a tree is too complex, it is prone to overfitting and must be used carefully with noisy data. For improved performance other variants might be considered like Random Forest and Gradient Boosted Trees.
  
```{r echo=TRUE, warning=FALSE}
dt_model <- rpart(class ~ ., data=train_set)
dt_pred <- round(predict(dt_model, newdata=test_set))
dt_acc <- sum(dt_pred == test_set$class) / nrow(test_set)
```

```{r include=FALSE, echo=TRUE, warning=FALSE}
# create confusion matrix
dt_cm <- confusionMatrix(table(factor(test_set$class), dt_pred))

# Get overall accuracy and F1 scores
dt_acc <- dt_cm$overall['Accuracy']
dt_f1_c0 <- dt_cm$byClass[, 'F1'][1]
dt_f1_c1 <- dt_cm$byClass[, 'F1'][2]
dt_f1_c2 <- dt_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Decision Tree", Accuracy=dt_acc, "F1 Class0"=dt_f1_c0, "F1 Class1"=dt_f1_c1, "F1 Class2"=dt_f1_c2))
```

Even better accuracy `r dt_acc` than with KNN, and the F1 scores are overall better and all > 0.916.

`r kable(ml_results)`  

## Random Forest
Random Forest combine multiple Decision Trees into a single model by randomly selecting a subset of data and features for each tree and combining the predictions (majority voting).

Random Forest corrects the habit of Decision Trees to ovefitting and are more robust with noisy data. They generally perform better than Decision Trees but with lower accuracy than Gradient Boosted Trees. But the additional complexity comes at the cost of more computational power required. They also don't perform well where the feature correlation is high or unbalanced.
  
```{r echo=TRUE, warning=FALSE}
rf_model <- ranger(class ~ ., data=train_set, num.trees=480)
rf_pred <- factor(round(predict(rf_model, data = test_set)$predictions), levels = c(0, 1, 2))
rf_acc <- sum(rf_pred == test_set$class) / nrow(test_set)
```

```{r include=FALSE, echo=TRUE, warning=FALSE}
# Calculate F1 score
rf_cm <- confusionMatrix(table(factor(test_set$class), rf_pred))

# Get overall accuracy and F1 scores
rf_acc <- rf_cm$overall['Accuracy']
rf_f1_c0 <- rf_cm$byClass[, 'F1'][1]
rf_f1_c1 <- rf_cm$byClass[, 'F1'][2]
rf_f1_c2 <- rf_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Random Forest", Accuracy=rf_acc, "F1 Class0"=rf_f1_c0, "F1 Class1"=rf_f1_c1, "F1 Class2"=rf_f1_c2))
```

Increasing the number of trees (to about 500) increases the model performance slightly but too high and the accuracy and F1 scores will come down again. While accuracy has gone up to `r rf_acc`, the average F1 score has come down slightly but still > 0.95.

`r kable(ml_results)`  
    
## Gradient Boosting
Gradient Boosting iteratively new weak learners (Decision Trees) to correct errors made by previous ones. Each iteration the negative gradient of the loss function is calculated. A new weak learner (Decision Tree) is fitted.  
Several tuning parameters are available like learning rate (determines the step size), number of weak learners and depth of the Decision Trees.
Gradient Boosting can be computationally expensive and prone to overfitting if the number of weak learners or the complexity of the individual trees is too high. 
Early stopping, regularization, and subsampling can be used to improve the performance and stability of the algorithm.
  
```{r echo=TRUE, warning=FALSE}
gb_model <- xgboost(data = as.matrix(train_set[, -1]),
                    label = train_set$class,
                    nrounds = 3,
                    objective = "multi:softmax",
                    num_class = 3,
                    eval_metric = "merror",
                    verbose = 0)

# convert predictions to integer class labels
gb_pred <- factor(predict(gb_model, as.matrix(test_set[, -1])), levels=c(0,1,2))
```

```{r include=FALSE, echo=TRUE, warning=FALSE}
# Confusion matrix
gb_cm <- confusionMatrix(table(factor(test_set$class), gb_pred))

# Get overall accuracy and F1 scores
gb_acc <- gb_cm$overall['Accuracy']
gb_f1_c0 <- gb_cm$byClass[, 'F1'][1]
gb_f1_c1 <- gb_cm$byClass[, 'F1'][2]
gb_f1_c2 <- gb_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Gradient Boost", Accuracy=gb_acc, "F1 Class0"=gb_f1_c0, "F1 Class1"=gb_f1_c1, "F1 Class2"=gb_f1_c2))
```

Well we got the "ultimate" accuracy of `r gb_acc` and all the F1 scores also at the maximum.

`r kable(ml_results)`  

This is extremely unlikely to happen in practice, so there might be some problems:

* Overfitting
* Dataset is too small 
* Dataset is too simple
* Data leakage (training data is used in verification)

At the moment the Gradient Boosting results must be excluded until the underlying problem is identified and fixed.

# Conclusion
In conclusion, the use of machine learning models has proved to be an effective tool for identifying and distinguishing between benign programs and malware. The performance of the models varies, with some achieving better accuracy than others, and some requiring additional tuning of parameters to improve their performance. However, overall, the models that have been tested show promising results in terms of their ability to accurately identify malware and benign programs.

Furthermore, the use of machine learning in cybersecurity has become increasingly important in recent years, as the number of cyber threats and attacks continues to rise. As technology advances, the methods used by cybercriminals to carry out attacks become more sophisticated and difficult to detect. Therefore, the use of machine learning models, along with other security measures, is essential in preventing and mitigating the impact of cyber attacks.

Overall, the results obtained from the use of machine learning models in identifying malware and benign programs are encouraging. Further research and development in this area will undoubtedly lead to even more effective tools for combating cyber threats, and the use of machine learning will continue to play a crucial role in cybersecurity.

While some models perform not or only slightly better than guessing, like Naive Bayes and SVM. Others achieve very good performance while stay balanced, like KNN, Decision Tree and Random Forest. For some of the models some additional tuning of parameters might be beneficial.
We are able to correctly identify between benign programs and Malware (static or dynamic) with > 95% accuracy and also good F1 scores:

`r kable(ml_results[ml_results$Model == "Random Forest", ])`

## Future Improvements
Bigger datasets like the one from Microsoft used for the [@https://doi.org/10.48550/arxiv.1802.10135] ["Microsoft Malware Classification Challenge"](https://arxiv.org/pdf/1802.10135.pdf) with more than 20'000 malware samples. The malwares are also finer classified into 9 different families and types (e.g. Worm, Adware, Backdoor, Trojan, TrojanDownloader, ...). Since the Microsoft dataset includes the file contents in hex format, lots of work would have been allocated to preprocessing the files. Also the dataset is quite popular in the cybersecurity research community with over 50 research papers and thesis works citing the dataset, this would violate the Capstone rules.

The models are not very refined and some tuning might increase the Accuracy (or F1 Score for the matter).

The goal might be to only identify two classes, benign and Malware. For now it was interessting to see if we can classify the Malware even further into dynamic and static, but in real world applications, this might not be of interesst.

Extracting other features from the original hex-files could provide more unique features. For this a more in-depth look at the dissassembly of these malwares is required. Also some feature combination or "pools" could be interessting, like the GUI or Menu relations or Events the program is using/triggering.

Some model approaches were not tested or lead to faulty results: 

* Neuronal Network
* Gradient Descent
* Gradient Boosting

\newpage
# System
## Hardware
```{r include=FALSE}
cpu_info <- get_cpu()
ram_info <- get_ram()
ram_formatted <- paste0(format(round(as.numeric(ram_info) / 1024^3, 2), nsmall = 2), " GB")
```
All above computations are done with an `r { cpu_info$model_name }` CPU with `r { cpu_info$no_of_cores }` and `r { ram_formatted }` of RAM.

## Software
This report is compiled using R markdown with RStudio.
`r sessionInfo() `

# Resources
[1] Rafael Irizarry. 2018. Introduction to Data Science.<https://rafalab.dfci.harvard.edu/dsbook/>

[2] Malware static and dynamic features VxHeaven and Virus Total Data Set <https://archive.ics.uci.edu/ml/datasets/Malware+static+and+dynamic+features+VxHeaven+and+Virus+Total#>

[3] PE Format <https://learn.microsoft.com/en-us/windows/win32/debug/pe-format>

[4] cuckoo sandbox <https://cuckoosandbox.org/>
