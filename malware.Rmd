---
title: "Report on Malware classification"
subtitle: "HarvardX Data Science Capstone Project"
author: "Raphael Kummer"
date: "`r format(Sys.Date())`"
output: 
  pdf_document:
    df_print: kable
    toc: yes
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, progress = TRUE, verbose = TRUE)

if(!require(pacman)) install.packages("pacman")
library(pacman)
p_load(ggplot2, benchmarkme, rmarkdown, lubridate, scales, parallel, stringr, kableExtra, tidyverse, caret, readr, httr, e1071, class, ranger, rpart, xgboost, nnet, glmnet, GGally)
```
\newpage
# Malware
## Introduction
Malicious software (Malware) is a software used to harm, damage, access or alter computer systems or networks. There are several types of Malware, including viruses, worms, trojan horses, ransomware and spyware. Most malware access computer systems by internet and emails. They can hide in legitimate applications or are hidden in the system.
Static analyses can be effective, but also easily evaded through obfuscation and altering the malware so much, that it is no longer recognized. Therefore additional dynamic features are used to classify malwares.

### Dataset
The dataset *Malware static and dynamic features VxHeaven and Virus Total Data Set* [2] from the UCI Machine Learning Repository contains three csv. 

* staDynBenignLab.csv: 1086 features extracted from 595 files on MS Windows 7 and 8, obtained Program Files directory.
* staDynVxHeaven2698Lab.csv: 1087 features extracted from 2698 files of VxHeaven dataset.
* staDynVt2955Lab.csv: 1087 features extracted from 2955 provided by Virus Total in 2018.

The staDynBenignLab has all the features of typical non-threatening programs wheare as staDynVxHeaven2698Lab and staDynVt2955Lab are extracted features of malware programs from the databases of VxHeaven and Virus Total. 
The dataset provides static features like ASM, compiler version, operating system version, Hex dump and PE header [3] (portable executable) and dynamic features extracted from a cuckoo sandbox [4].
The dataset only contains windows specific programs and malware.
The dataset is downloaded from the UCI Machine Learning Repository Dataset and the containing csv files are extracted. The data in each of the csv must be labeled first, describing the class of program identified (e.g. class 0 for benign, class 1 for static malware features and class 2 for dynamic malware features).

## Goal
The dataset is used to explore and gain insight on how normal programs and malware differ in static and dynamic structure. Several machine learning algorithms to classify an unseen program as benign or malware (static or dynamic) are developed and compared.
Overall, the goal of this project is to improve our understanding of how malware differs from normal programs, and to develop effective techniques for automatically identifying and classifying malware based on its static and dynamic features.

## Summary
We are able to correctly identify between benign programs and Malware (static or dynamic) with > 95% accuracy and also good F1 scores with the Random Forest model.

# Analysis
Disclaimer: Not all R code will be shown in this report. For the full code view the R-File on github.

## Download
The zipped dataset is downloaded from the UCI archive and unzipped. 
  
```{r include=FALSE, echo=FALSE, warning=FALSE}
# download dataset from UCI archive https://archive.ics.uci.edu/ml/datasets/Malware+static+and+dynamic+features+VxHeaven+and+Virus+Total#
download_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00541/datasets_DetectingMalwareUsingAHybridApproach.zip'
download.file(download_url, 'datasets.zip', mode='wb')

# unzip
unzip_result <- unzip("datasets.zip", overwrite = TRUE, exdir="data")
```
  
This will extract the following files: `r list.files("data")`

```{r include=FALSE, echo=FALSE, warning=FALSE}
 # load these csv
benign <- read_csv("data/staDynBenignLab.csv")
vxheaven <- read_csv("data/staDynVxHeaven2698Lab.csv")
vt <- read_csv("data/staDynVt2955Lab.csv")
```
 
Where 

* *staDynBenignLab.csv*has `r { ncol(benign) }` features extracted from `r { nrow(benign) }` files on MS Windows 7 and 8, obtained Program Files directory
* *staDynVxHeaven2698Lab.csv* has `r { ncol(vxheaven) }` features extracted from `r { nrow(vxheaven) }` files of VxHeaven dataset.
* *staDynVt2955Lab.csv* has `r { ncol(vt) }` features extracted from `r { nrow(vt) }` provided by Virus Total in 2018.

## Preprocessing
Add classes to each dataset, depending on the origin of their data:
      
* 0: Normal *benign* programs 
* 1: Malware with static features extracted from the *VxHeaven (vxheaven)* dataset
* 2: Malware with dynamic features extracted from the *Virus Total (vt)* dataset

```{r echo=FALSE}
benign$class <- 0
vxheaven$class <- 1
vt$class <- 2
```
  
## Cleanup
The datasets have 

* staDynBenignLab: `r sum(is.na(benign))`, 
* staDynVxHeaven2698Lab: `r sum(is.na(vxheaven))` and 
* staDynVt2955Lab: `r sum(is.na(vt))` 

missing values.

Check for features that not present in all three datasets, remove unique columns and combine all in one large dataset. 
```{r echo=FALSE}   
# list columns of all datasets
cols_benign <- colnames(benign)
cols_vxheaven <- colnames(vxheaven)
cols_vt <- colnames(vt)

# Find common column names
common_cols <- intersect(intersect(cols_benign, cols_vxheaven), cols_vt)

# Find different column names
different_cols <- setdiff(union(union(cols_benign, cols_vxheaven), cols_vt), common_cols)

# remove unique columns
benign <- benign[, common_cols]
vxheaven <- vxheaven[, common_cols]
vt <- vt[, common_cols]
# now each dataset has 1085 features

## classify each dataset
#benign$class <- 0
#vxheaven$class <- 1
#vt$class <- 2
      
# create one large dataset 
data <- rbind(benign, vxheaven, vt)
```
These columns are not present in all datasets:

*`r different_cols`*

So they are removed and the resulting dataset we now work with has `r { ncol(benign) }` features for each of the `r { nrow(benign) }` different programs.

## Variance
Check feature variance for features without any relevant information, find all these features where the variation is 0 and remove them from the dataset.

```{r echo=TRUE}
# Calculate the variance of each variable in the dataset
variances <- apply(data, 2, var)

# Find which variances are equal to 0
zero_variances <- which(variances == 0)
```

`r { length(zero_variances)}` found to have a variance of 0. E.g. all variables of "count_file_renamed" are 0 so there is no benefit for the analysis or training of models. These are removed from the dataset and the remaining variances are plotted here:

```{r echo=TRUE}
# Exclude any variables with 0 variance
nonzero_variances <- variances[variances > 0]
# only 246 variables remain

# Create a bar plot of the variances
barplot(nonzero_variances, main="Log Variance", las=2, log="y", xlab="var", ylab="variance")
```

```{r include=FALSE}
# create a dataset with only relevant variables
nonzero_cols <- colnames(data)[variances != 0]
data_v2 <- data[, nonzero_cols]
```
## Feature Clusters
### File/exe charakteristics
### Compiler/Linker charakteristics
### Imports
### Datatypes
### ASM and Functions used
### GUI and Menu
### Events
### System
### DLL

## Inspection

  
\newpage
# Models
Test several machine learning approaches to classify unseen programs (according to their static and dynamic features) as benign or possibly malicious, differenciate between identified according to static or dynamic features.
The dataset is split into two parts, one for training and anoter for testing the model. The proportion of the data allocated to the test set is 30% of the complete data, the other 70% are used for the training set.

The goal is to achieve a high accuracy (>90%) with balanced precision and recall (F1) over all classes. 

```{r echo=FALSE, warning=FALSE}
set.seed(42) # because 42 is always the answer

# split data into training and test set
test_index <- createDataPartition(y=data_v2$class, times=1, p=0.3, list=FALSE)
colnames(data_v2) <- make.names(colnames(data_v2))

train_set <- data_v2[-test_index,]
test_set <- data_v2[test_index,]

ml_results <- tibble()
``` 
  
## Guessing
The simplest approach, though not very usefull, would be by simply guessing the classification of a program. It is expected that the accuracy would be 33.3%. 

```{r echo=FALSE, warning=FALSE}
# "Train" the guess model, apply randomly 0,1 or 2
guess_model <- factor(sample(c(0, 1, 2), length(test_set$class), replace=TRUE), levels=c(0,1,2))
guess_acc <- sum(guess_model == test_set$class) / nrow(test_set)
# expected to be around 33.3%

# Create the confusion matrix
guess_cm <- confusionMatrix(table(factor(test_set$class), guess_model))

# Get overall accuracy and F1 scores
guess_acc <- guess_cm$overall['Accuracy']
guess_f1_c0 <- guess_cm$byClass[, 'F1'][1]
guess_f1_c1 <- guess_cm$byClass[, 'F1'][2]
guess_f1_c2 <- guess_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Guessing", Accuracy=guess_acc, "F1 Class0"=guess_f1_c0, "F1 Class1"=guess_f1_c1, "F1 Class2"=guess_f1_c2))
```
The achieved accuracy is `r guess_acc`, close to the expected 33.3%.
`r knitr::kable(ml_results)`
  
## Naive Bayes
Naive Bayes is a simple and fast algorithm often used for classifications. The algorithm is based on the Bayes' theorem, that the probability of an event might be related to the event based on knowledge of conditions. 

$P(A|B)=P(B|A)*P(A)/P(B)$

Naive means that the features are mutually independent, so a the probability of one feature does not affect the probability of another feature.
There are several Naive Bayes variants:
  
  * Gaussian Naive Bayes where the features are normally distributed
  * Bernoulli Naive Bayes where the features are binary
  * Multinomial Naive Bayes where frequency of occurences of features is calculated
  
Once the likelihood and prior probabilities have been calculated, Naive Bayes uses Bayes' theorem to compute the probability of each class label for a given set of feature values. The class label with the highest probability is then selected as the predicted label for these set of input data.
The algorithm can be effective for classifications when lots of features are involved, but performance may suffer when features are not independent or unbalanced distributed.
  
```{r echo=TRUE, warning=FALSE}
# Train the Naive Bayes model
nb_model <- naiveBayes(class ~ ., data=train_set)
nb_pred <- predict(nb_model, newdata=test_set)

# creater confusion matrix
nb_cm <- confusionMatrix(table(factor(test_set$class), nb_pred))

# Get overall accuracy and F1 scores
nb_acc <- nb_cm$overall['Accuracy']
nb_f1_c0 <- nb_cm$byClass[, 'F1'][1]
nb_f1_c1 <- nb_cm$byClass[, 'F1'][2]
nb_f1_c2 <- nb_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Naive Bayes", Accuracy=nb_acc, "F1 Class0"=nb_f1_c0, "F1 Class1"=nb_f1_c1, "F1 Class2"=nb_f1_c2))
```

The Naive Bayes is not a route to follow further, we get a very low overall accuracy `r nb_acc`, lower than guessing.

`r knitr::kable(ml_results)`

## SVM
Support Vector Machine (SVM) is a supervised machine learning algorithm used for regression or classification tasks. The algorithm tries to fit hyperplanes that separates the data into different classes. In the training finding the hyperplane with the largest margin (distance between the hyperplane and the closest data points from each class).  
In 2-D the hyperplane is basically a line, that separates the classes. Non-binary classification can be done by transforming the input features in higher dimensional space where it can be separated by hyperplanes.
    
SVM are powerfull if the data is clearly separated, but must be used carefully with noisy data or imbalanced classes.

```{r echo=TRUE, warning=FALSE}
# train SVM model
svm_model <- svm(class ~ ., data=train_set)
svm_pred <- factor(round(predict(svm_model, newdata=test_set)), levels=c(0,1,2))

# creater confusion matrix
svm_cm <- confusionMatrix(table(factor(test_set$class), svm_pred))

# Get overall accuracy and F1 scores
svm_acc <- svm_cm$overall['Accuracy']
svm_f1_c0 <- svm_cm$byClass[, 'F1'][1]
svm_f1_c1 <- svm_cm$byClass[, 'F1'][2]
svm_f1_c2 <- svm_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="SVM", Accuracy=svm_acc, "F1 Class0"=svm_f1_c0, "F1 Class1"=svm_f1_c1, "F1 Class2"=svm_f1_c2))
```
With `r svm_acc` we are now better than guessing, but the F1 scores are still a bit low.

`r kable(ml_results)`  

    
## KNN
K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for regression or classification tasks. It uses the proximity to grouping to make predictions of an individual point, assuming similar points can be found close to another. 

Small k values can lead to overfitting and the model becomes to sensitive to noise and outliers, whereas a large k may result in underfitting and the model does not capture patterns in the data.

The KNN algorithm will perform best with a small number of features. Therefore we don't expect have high hopes for this approach.
  
```{r echo=TRUE, warning=FALSE}
knn_model <- knn(train_set[, -ncol(train_set)], test_set[, -ncol(test_set)], train_set$class, k = 3)

# creater confusion matrix
knn_cm <- confusionMatrix(table(factor(test_set$class), knn_model))

# Get overall accuracy and F1 scores
knn_acc <- knn_cm$overall['Accuracy']
knn_f1_c0 <- knn_cm$byClass[, 'F1'][1]
knn_f1_c1 <- knn_cm$byClass[, 'F1'][2]
knn_f1_c2 <- knn_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="KNN", Accuracy=knn_acc, "F1 Class0"=knn_f1_c0, "F1 Class1"=knn_f1_c1, "F1 Class2"=knn_f1_c2))
```
Accuracy is now very good `r knn_acc`, and also the F1 scores around 0.9.

`r kable(ml_results)`  
    
## Decision Trees
Decision Trees can be used for regression and classification models. Each node in the tree represents a decision on feature which are followed until a leaf node is reached, which represents a class.
These trees are easy to visualize and interpret. But if a tree is too complex, it is prone to overfitting and must be used carefully with noisy data. For improved performance other variants might be considered like Random Forest and Gradient Boosted Trees.
  
```{r echo=TRUE, warning=FALSE}
dt_model <- rpart(class ~ ., data=train_set)
dt_pred <- round(predict(dt_model, newdata=test_set))
dt_acc <- sum(dt_pred == test_set$class) / nrow(test_set)

# create confusion matrix
dt_cm <- confusionMatrix(table(factor(test_set$class), dt_pred))

# Get overall accuracy and F1 scores
dt_acc <- dt_cm$overall['Accuracy']
dt_f1_c0 <- dt_cm$byClass[, 'F1'][1]
dt_f1_c1 <- dt_cm$byClass[, 'F1'][2]
dt_f1_c2 <- dt_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Decision Tree", Accuracy=dt_acc, "F1 Class0"=dt_f1_c0, "F1 Class1"=dt_f1_c1, "F1 Class2"=dt_f1_c2))
```
Even better accuracy `r dt_acc` than with KNN, and the F1 scores are overall better and all > 0.916.

`r kable(ml_results)`  

## Random Forest
Random Forest combine multiple Decision Trees into a single model by randomly selecting a subset of data and features for each tree and combining the predictions (majority voting).

Random Forest corrects the habit of Decision Trees to ovefitting and are more robust with noisy data. They generally perform better than Decision Trees but with lower accuracy than Gradient Boosted Trees. But the additional complexity comes at the cost of more computational power required. They also don't perform well where the feature correlation is high or unbalanced.
  
```{r echo=TRUE, warning=FALSE}
rf_model <- ranger(class ~ ., data=train_set, num.trees=480)
rf_pred <- factor(round(predict(rf_model, data = test_set)$predictions), levels = c(0, 1, 2))
rf_acc <- sum(rf_pred == test_set$class) / nrow(test_set)

# Calculate F1 score
rf_cm <- confusionMatrix(table(factor(test_set$class), rf_pred))

# Get overall accuracy and F1 scores
rf_acc <- rf_cm$overall['Accuracy']
rf_f1_c0 <- rf_cm$byClass[, 'F1'][1]
rf_f1_c1 <- rf_cm$byClass[, 'F1'][2]
rf_f1_c2 <- rf_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Random Forest", Accuracy=rf_acc, "F1 Class0"=rf_f1_c0, "F1 Class1"=rf_f1_c1, "F1 Class2"=rf_f1_c2))
```
Increasing the number of trees (to about 500) increases the model performance slightly but too high and the accuracy and F1 scores will come down again. While accuracy has gone up to `r rf_acc`, the average F1 score has come down slightly but still > 0.95.

`r kable(ml_results)`  
    
## Gradient Boosting
Gradient Boosting iteratively new weak learners (Decision Trees) to correct errors made by previous ones. Each iteration the negative gradient of the loss function is calculated. A new weak learner (Decision Tree) is fitted.  
Several tuning parameters are available like learning rate (determines the step size), number of weak learners and depth of the Decision Trees.
Gradient Boosting can be computationally expensive and prone to overfitting if the number of weak learners or the complexity of the individual trees is too high. 
Early stopping, regularization, and subsampling can be used to improve the performance and stability of the algorithm.
  
```{r echo=TRUE, warning=FALSE}
gb_model <- xgboost(data = as.matrix(train_set[, -1]),
                    label = train_set$class,
                    nrounds = 3,
                    objective = "multi:softmax",
                    num_class = 3,
                    eval_metric = "merror",
                    verbose = 0)

# convert predictions to integer class labels
gb_pred <- factor(predict(gb_model, as.matrix(test_set[, -1])), levels=c(0,1,2))

# Confusion matrix
gb_cm <- confusionMatrix(table(factor(test_set$class), gb_pred))

# Get overall accuracy and F1 scores
gb_acc <- gb_cm$overall['Accuracy']
gb_f1_c0 <- gb_cm$byClass[, 'F1'][1]
gb_f1_c1 <- gb_cm$byClass[, 'F1'][2]
gb_f1_c2 <- gb_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Gradient Boost", Accuracy=gb_acc, "F1 Class0"=gb_f1_c0, "F1 Class1"=gb_f1_c1, "F1 Class2"=gb_f1_c2))
```
Well we got the "ultimate" accuracy of `r gb_acc` and all the F1 scores also at the maximum.
`r kable(ml_results)`  

This is extremely unlikely to happen in practice, so there might be some problems:

* Overfitting
* Dataset is too small 
* Dataset is too simple
* Data leakage (training data is used in verification)

At the moment the Gradient Boosting results must be excluded until the underlying problem is identified and fixed.

# Conclusion
While some models perform not or only slightly better than guessing, like Naive Bayes and SVM. Others achieve very good performance while stay balanced, like KNN, Decision Tree and Random Forest. For some of the models some additional tuning of parameters might be beneficial.
We are able to correctly identify between benign programs and Malware (static or dynamic) with > 95% accuracy and also good F1 scores:

`r kable(ml_results[ml_results$Model == "Random Forest", ])`

  
## Future Improvements
Bigger datasets like the one from Microsoft used for the [@https://doi.org/10.48550/arxiv.1802.10135] ["Microsoft Malware Classification Challenge"](https://arxiv.org/pdf/1802.10135.pdf) with more than 20'000 malware samples. The malwares are also finer classified into 9 different families and types (e.g. Worm, Adware, Backdoor, Trojan, TrojanDownloader, ...). Since the Microsoft dataset includes the file contents in hex format, lots of work would have been allocated to preprocessing the files. Also the dataset is quite popular in the cybersecurity research community with over 50 research papers and thesis works citing the dataset, this would violate the Capstone rules.

The models are not very refined and some tuning might increase the Accuracy (or F1 Score for the matter).

The goal might be to only identify two classes, benign and Malware. For now it was interessting to see if we can classify the Malware even further into dynamic and static, but in real world applications, this might not be of interesst.

Extracting other features from the original hex-files could provide more unique features. For this a more in-depth look at the dissassembly of these malwares is required. Also some feature combination or "pools" could be interessting, like the GUI or Menu relations or Events the program is using/triggering.

Some model approaches were not tested or lead to faulty results: 

* Neuronal Network
* Gradient Descent
* Gradient Boosting

\newpage
# System
## Hardware
```{r include=FALSE}
cpu_info <- get_cpu()
ram_info <- get_ram()
ram_formatted <- paste0(format(round(as.numeric(ram_info) / 1024^3, 2), nsmall = 2), " GB")
```
All above computations are done with an `r { cpu_info$model_name }` CPU with `r { cpu_info$no_of_cores }` and `r { ram_formatted }` of RAM.


## Software
This report is compiled using R markdown with RStudio.
```{r}
sessionInfo()
```

# Resources
[1] Rafael Irizarry. 2018. Introduction to Data Science.<https://rafalab.dfci.harvard.edu/dsbook/>

[2] Malware static and dynamic features VxHeaven and Virus Total Data Set <https://archive.ics.uci.edu/ml/datasets/Malware+static+and+dynamic+features+VxHeaven+and+Virus+Total#>

[3] PE Format <https://learn.microsoft.com/en-us/windows/win32/debug/pe-format>

[4] cuckoo sandbox <https://cuckoosandbox.org/>
