---
title: "Report on Malware classification"
subtitle: "HarvardX Data Science Capstone Project"
author: "Raphael Kummer"
date: "`r format(Sys.Date())`"
output: 
  pdf_document:
    df_print: kable
    toc: yes
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, progress = TRUE, verbose = TRUE)

install.packages("pacman") # if not already installed
library(pacman)
p_load(ggplot2, benchmarkme, rmarkdown, lubridate, scales, parallel, stringr, kableExtra, tidyverse, caret, readr, httr, e1071, class, ranger, rpart, xgboost, nnet, glmnet, GGally)
```
\newpage
# Malware
## Introduction
Malicious software (Malware) is a software used to harm, damage, access or alter computer systems or networks. There are several types of Malware, including viruses, worms, trojan horses, ransomware and spyware. Most malware access computer systems by internet and emails. They can hide in legitimate applications or are hidden in the system.
Static analyses can be effective, but also easily evaded through obfuscation and altering the malware so much, that it is no longer recognized. Therefore additional dynamic features are used to classify malwares.

### Dataset
The dataset *Malware static and dynamic features VxHeaven and Virus Total Data Set* [2] from the UCI Machine Learning Repository contains three csv. 

* staDynBenignLab.csv: 1086 features extracted from 595 files on MS Windows 7 and 8, obtained Program Files directory.
* staDynVxHeaven2698Lab.csv: 1087 features extracted from 2698 files of VxHeaven dataset.
* staDynVt2955Lab.csv: 1087 features extracted from 2955 provided by Virus Total in 2018.

The staDynBenignLab has all the features of typical non-threatening programs wheare as staDynVxHeaven2698Lab and staDynVt2955Lab are extracted features of malware programs from the databases of VxHeaven and Virus Total. 
The dataset provides static features like ASM, compiler version, operating system version, Hex dump and PE header [3] (portable executable) and dynamic features extracted from a cuckoo sandbox [4].
The dataset only contains windows specific programs and malware.

## Initial setup
Download the dataset from the UCI Machine Learning Repository Dataset and extract the containing csv files. Add a label to each of the csv, describing the class of program identified (e.g. class 0 for benign, class 1 for static malware features and class 2 for dynamic malware features).

## Goal
The dataset is used to explore and gain insight on how normal programs and malware differ in static and dynamic structure. Several machine learning algorithms to classify an unseen program as benign or malware (static or dynamic) are developed and compared.
Overall, the goal of this project is to improve our understanding of how malware differs from normal programs, and to develop effective techniques for automatically identifying and classifying malware based on its static and dynamic features.

## Summary
<TODO>

# Analysis
## Download
The zipped dataset is downloaded from the UCI archive and unzipped. 
  
```{r echo=FALSE, warning=FALSE}
# download dataset from UCI archive https://archive.ics.uci.edu/ml/datasets/Malware+static+and+dynamic+features+VxHeaven+and+Virus+Total#
download_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00541/datasets_DetectingMalwareUsingAHybridApproach.zip'
download.file(download_url, 'datasets.zip', mode='wb')

# unzip
unzip_result <- unzip("datasets.zip", overwrite = TRUE)
```
  
This will extract the following files:
```{r echo=TRUE}
list.files()
  
 # load these csv
benign <- read_csv("staDynBenignLab.csv")
vxheaven <- read_csv("staDynVxHeaven2698Lab.csv")
vt <- read_csv("staDynVt2955Lab.csv")
 ```
Where 
- *staDynBenignLab.csv* has `r { ncol(benign) }` features extracted from `r { nrow(benign) }` files on MS Windows 7 and 8, obtained Program Files directory
- *staDynVxHeaven2698Lab.csv* has `r { ncol(vxheaven) }` features extracted from `r { nrow(vxheaven) }` files of VxHeaven dataset.
- *staDynVt2955Lab.csv* has `r { ncol(vt) }` features extracted from `r { nrow(vt) }` provided by Virus Total in 2018.

## Preprocessing
Add classes to each dataset, depending on the origin of their data:
      
0: Normal *benign* programs 
1: Malware with static features extracted from the *VxHeaven (vxheaven)* dataset
2: Malware with dynamic features extracted from the *Virus Total (vt)* dataset

```
# classify each dataset
benign$class <- 0
vxheaven$class <- 1
vt$class <- 2
```
  
## Cleanup
The datasets have staDynBenignLab: `r sum(is.na(benign))`, staDynVxHeaven2698Lab: `r sum(is.na(vxheaven))` and staDynVt2955Lab: `r sum(is.na(vt))` missing values.

Check for features that not present in all three datasets, remove unique columns and combine all in one large dataset. 
```{r echo=FALSE}     
# remove unique columns
benign <- benign[, common_cols]
vxheaven <- vxheaven[, common_cols]
vt <- vt[, common_cols]
# now each dataset has 1085 features
      
# create one large dataset 
data <- rbind(benign, vxheaven, vt)
```
The resulting dataset we now work with has `r { ncol(benign) }` features for each of the `r { nrow(benign) }` different programs.

## Variance
Check feature variance for features without any relevant information, find all these features where the variation is 0 and remove them from the dataset.

```{r echo=TRUE}
# Calculate the variance of each variable in the dataset
variances <- apply(data, 2, var)

# Find which variances are equal to 0
zero_variances <- which(variances == 0)
```

`r { length(zero_variances)}` found to have a variance of 0. E.g. all variables of "count_file_renamed" are 0 so there is no benefit for the analysis or training of models. These are removed from the dataset and the remaining variances are plotted here:

```{r echo=TRUE}
# Exclude any variables with 0 variance
nonzero_variances <- variances[variances > 0]
# only 246 variables remain

# Create a bar plot of the variances
barplot(nonzero_variances, main="Log Variance", las=2, log="y", xlab="var", ylab="variance")
```

## Feature Clusters
### File/exe charakteristics
### Compiler/Linker charakteristics
### Imports
### Datatypes
### ASM and Functions used
### GUI and Menu
### Events
### System
### DLL

## Inspection

  

# Models
Test several machine learning approaches to classify unseen programs (according to their static and dynamic features) as benign or possibly malicious, differenciate between identified according to static or dynamic features.
The dataset is split into two parts, one for training and anoter for testing the model. The proportion of the data allocated to the test set is 30% of the complete data, the other 70% are used for the training set.

```{r echo=FALSE}
set.seed(42) # because 42 is always the answer

# split data into training and test set
test_index <- createDataPartition(y = data$class, times=1, p=0.3, list=FALSE)
colnames(data) <- make.names(colnames(data))

train_set <- data[-test_index,]
test_set <- data[test_index,]

ml_results <- tibble()
``` 
  
## Guessing
The simplest approach, though not very usefull, would be by simply guessing the classification of a program. It is expected that the accuracy would be 33.3%. 

```{r echo=FALSE}
guess_model <- factor(sample(c(0, 1, 2), length(test_set$class), replace=TRUE), levels=c(0,1,2))
guess_acc <- sum(guess_model == test_set$class) / nrow(test_set)
# expected to be around 33.3%

# Calculate F1 score
guess_cm <- confusionMatrix(table(factor(test_set$class), guess_model))
guess_f1 <- guess_cm$byClass[4]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Guessing", Accuracy=guess_acc, F1=guess_f1))
```
The accived accuracy is `r guess_acc`, close to the expected 33.3%.
`r knitr::kable(ml_results)`
  
## Naive Bayes
Naive Bayes is a simple and fast algorithm often used for classifications. The algorithm is based on the Bayes' theorem, that the probability of an event might be related to the event based on knowledge of conditions. 
$P(A|B)=P(B|A)*P(A)/P(B)$
Naive means that the features are mutually independent, so a the probability of one feature does not affect the probability of another feature.
There are several Naive Bayes variants:
  
  * Gaussian Naive Bayes where the features are normally distributed
  * Bernoulli Naive Bayes where the features are binary
  * Multinomial Naive Bayes where frequency of occurences of features is calculated
  
Once the likelihood and prior probabilities have been calculated, Naive Bayes uses Bayes' theorem to compute the probability of each class label for a given set of feature values. The class label with the highest probability is then selected as the predicted label for these set of input data.
The algorithm can be effective for classifications when lots of features are involved, but performance may suffer when features are not independent or unbalanced distributed.
  
```{r echo=TRUE}
nb_model <- naiveBayes(class ~ ., data=train_set)
nb_pred <- predict(nb_model, newdata=test_set)
nb_acc <- sum(nb_pred == test_set$class) / nrow(test_set)

# Calculate F1 score
nb_cm <- confusionMatrix(table(factor(test_set$class), nb_pred))
nb_f1 <- nb_cm$byClass[4]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Naive Bayes", Accuracy=nb_acc, F1=nb_f1))
  
kable(ml_results)
```

  <TODO>
  
## SVM
  SVM (Support Vector Machine) is a supervised machine learning algorithm that is commonly used for classification tasks. The main idea behind SVM is to find a hyperplane that separates the data into different classes in a way that maximizes the margin between the hyperplane and the data points that are closest to it, which are called support vectors.

In a two-class classification problem, the hyperplane can be described by the equation:

$f(x) = w^T x + b$

where w is a weight vector, x is the input feature vector, b is the bias term, and f(x) is the predicted class label (+1 or -1). The goal of SVM is to find the values of w and b that maximize the margin between the hyperplane and the support vectors.

The margin is the distance between the hyperplane and the closest data points from each class. The idea is to find the hyperplane that maximizes this margin while also ensuring that all data points are classified correctly. This is achieved by solving an optimization problem that involves minimizing the weight vector subject to the constraint that all data points are classified correctly and that the margin is maximized.

SVM can also be extended to handle non-linearly separable data by using kernel functions to transform the input features into a higher-dimensional space where the data becomes separable by a hyperplane.

Overall, SVM is a powerful algorithm that can handle both linear and non-linearly separable data and is particularly useful when dealing with high-dimensional feature spaces. However, SVM can be sensitive to the choice of hyperparameters and may not perform well when the data is noisy or when the classes are heavily imbalanced.

```{r echo=FALSE}
svm_model <- svm(class ~ ., data=train_set)
svm_pred <- factor(round(predict(svm_model, newdata=test_set)), levels=c(0,1,2))
svm_acc <- sum(svm_pred == test_set$class) / nrow(test_set)

svm_cm <- confusionMatrix(table(factor(test_set$class), svm_pred))
svm_f1 <- svm_cm$byClass[4]
          
ml_results <- ml_results %>%
  bind_rows(tibble(Model="SVM", Accuracy=svm_acc, F1=svm_f1))
  
kable(ml_results)
```
## KNN
KNN (K-Nearest Neighbors) is a simple and popular supervised machine learning algorithm used for both classification and regression tasks. The idea behind KNN is to find the k closest data points to a new, unseen data point in the feature space, and use the labels of these nearest neighbors to predict the label of the new data point.

For classification tasks, the label of the new data point is predicted as the majority class label among the k nearest neighbors. For regression tasks, the predicted value is the average of the k nearest neighbor values.

The choice of the value of k is an important hyperparameter in KNN. A small value of k can lead to overfitting, where the model may be too sensitive to noise and outliers in the data. A large value of k, on the other hand, may lead to underfitting, where the model may not capture the underlying patterns in the data.

The distance metric used to measure the distance between the new data point and its neighbors is another important factor in KNN. Euclidean distance is commonly used for continuous features, while Hamming distance or Jaccard distance may be used for categorical features.

One potential issue with KNN is that it can be sensitive to the scaling of the features. If some features have larger magnitudes than others, they may dominate the distance calculation and bias the predictions towards those features.

Overall, KNN is a simple and effective algorithm for classification and regression tasks, particularly when the decision boundary is nonlinear and the number of features is small. However, its performance can be sensitive to the choice of hyperparameters and the scaling of the features.
Therefore we don't expect KNN to perform very well.
  
```{r echo=FALSE}
knn_model <- knn(train_set[, -ncol(train_set)], test_set[, -ncol(test_set)], train_set$class, k = 3)
knn_acc <- sum(knn_model == test_set$class) / nrow(test_set)

# Calculate F1 score
knn_cm <- confusionMatrix(table(factor(test_set$class), knn_model))
knn_f1 <- knn_cm$byClass[4]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="KNN", Accuracy=knn_acc, F1=knn_f1)) 

kable(ml_results)
```
  
## Decision Trees
Decision Trees are a popular and intuitive machine learning algorithm used for both classification and regression tasks. They are based on a tree-like model of decisions and their possible consequences, where each internal node represents a test on a feature, each branch represents the outcome of the test, and each leaf node represents a class label or a predicted value.

The main idea behind Decision Trees is to recursively split the data into subsets that are as homogeneous as possible with respect to the class label or the target variable. The splitting criterion used to select the best feature and split point depends on the type of data and the task. For classification tasks, common splitting criteria include Gini impurity, entropy, and classification error. For regression tasks, common splitting criteria include mean squared error and mean absolute error.

During the training phase, the Decision Tree algorithm searches the feature space for the best splits that maximize the homogeneity of the resulting subsets. The process continues until a stopping criterion is met, such as a maximum tree depth, a minimum number of samples per leaf, or a maximum impurity reduction.

Once the Decision Tree has been constructed, it can be used to make predictions on new data by following the decision path from the root node to a leaf node that corresponds to the predicted class label or target value.

Decision Trees have several advantages, such as being easy to interpret and visualize, and being able to handle both categorical and numerical features. Additionally, Decision Trees can capture nonlinear and interaction effects between the features, making them suitable for complex and nonlinear datasets.

However, Decision Trees can also suffer from several limitations, such as being prone to overfitting if the tree is too complex, being sensitive to the choice of splitting criterion and the ordering of the features, and being unstable if the training data is noisy or contains outliers. To address these issues, ensemble methods such as Random Forest and Gradient Boosted Trees are often used to improve the performance of Decision Trees.
  
```{r echo=TRUE}
dt_model <- rpart(class ~ ., data=train_set)
dt_pred <- round(predict(dt_model, newdata=test_set))
dt_acc <- sum(dt_pred == test_set$class) / nrow(test_set)

# Calculate F1 score
dt_cm <- confusionMatrix(table(factor(test_set$class), dt_pred))
dt_f1 <- dt_cm$byClass[4]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Decision Trees", Accuracy=dt_acc, F1=dt_f1))
  
kable(ml_results)  
```
  
## Random Forest
  Random Forest is an ensemble learning algorithm that is commonly used for classification and regression tasks. The main idea behind Random Forest is to combine multiple decision trees into a single model by randomly selecting subsets of the data and features for each tree and then aggregating their predictions.

To build a Random Forest model, the algorithm first creates a set of decision trees using a bootstrapped sample of the training data. For each tree, a random subset of the features is selected at each split to reduce correlation among the trees and to promote feature diversity.

During the training phase, each decision tree is grown using a greedy algorithm that selects the best feature and split point to minimize the impurity of the resulting node. The impurity measure used for classification tasks is typically Gini impurity or entropy, while for regression tasks it is typically mean squared error.

Once all the trees have been grown, the Random Forest algorithm aggregates their predictions by either taking the majority vote (for classification tasks) or the average (for regression tasks) of the predictions across all trees.

One of the main advantages of Random Forest is that it is a highly accurate and robust algorithm that can handle noisy and high-dimensional data. Additionally, Random Forest can provide important information on the relative importance of the different features in the data, making it useful for feature selection and interpretation.

However, Random Forest can be computationally expensive and may require tuning of several hyperparameters, such as the number of trees, the maximum depth of each tree, and the size of the feature subsets. Additionally, Random Forest may not perform well on datasets with highly correlated features or unbalanced class distributions.
  
```{r echo=TRUE}
rf_model <- ranger(class ~ ., data=train_set, num.trees=100)
rf_pred <- factor(round(predict(rf_model, data = test_set)$predictions), levels = c(0, 1, 2))
rf_acc <- sum(rf_pred == test_set$class) / nrow(test_set)

# Calculate F1 score
rf_cm <- confusionMatrix(table(factor(test_set$class), rf_pred))
rf_f1 <- rf_cm$byClass[4]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Random Forest", Accuracy=rf_acc, F1=rf_f1))
  
kable(ml_results)
```

## Gradient Boosting

Gradient Boosting is a powerful machine learning algorithm that is widely used for both regression and classification tasks. It is an ensemble learning method that combines multiple weak learners (typically decision trees) to create a strong predictive model.

The main idea behind Gradient Boosting is to iteratively train new weak learners to correct the errors made by the previous ones. In each iteration, the algorithm calculates the negative gradient of the loss function with respect to the predicted values, and then fits a new weak learner to the residual errors. The new learner is added to the ensemble, and its predictions are combined with the predictions of the previous learners using a weighted sum.

The key to the success of Gradient Boosting is the use of a gradient-based optimization algorithm to update the weights of the weak learners. By minimizing the loss function in the direction of the negative gradient, the algorithm is able to find a series of weak learners that together can approximate the true underlying function.

There are several hyperparameters that can be tuned to control the performance and complexity of the Gradient Boosting model, such as the learning rate, which determines the step size in the gradient descent algorithm, the number of weak learners, and the maximum depth of the decision trees.

Gradient Boosting has several advantages over other machine learning algorithms, such as its ability to handle complex and nonlinear relationships between the features and the target variable, its robustness to noise and outliers in the data, and its ability to handle missing values and categorical features.

However, Gradient Boosting can also be computationally expensive and prone to overfitting if the number of weak learners or the complexity of the individual trees is too high. To address these issues, techniques such as early stopping, regularization, and subsampling can be used to improve the performance and stability of the algorithm.
  
```{r echo=TRUE}
gb_model <- xgboost(data = as.matrix(train_set[, -1]),
                    label = train_set$class,
                    nrounds = 100,
                    objective = "multi:softmax",
                    num_class = 3,
                    eval_metric = "merror")

gb_pred <- factor(predict(gb_model, as.matrix(test_set[, -1])), levels=c(0,1,2))
# convert predictions to integer class labels
gb_acc <- sum(gb_pred == test_set$class) / nrow(test_set)
gb_cm <- confusionMatrix(table(factor(test_set$class), gb_pred))

gb_f1 <- gb_cm$byClass[4]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Gradient Boosting", Accuracy=gb_acc, F1=gb_f1))
  
kable(ml_results)
```
  
# Conclusion
  
## Future Improvements
Bigger datasets like the one from Microsoft used for the ["Microsoft Malware Classification Challenge"](https://arxiv.org/pdf/1802.10135.pdf) with more than 20'000 malware samples. The malwares are also finer classified into 9 different families and types (e.g. Worm, Adware, Backdoor, Trojan, TrojanDownloader, ...). Since the Microsoft dataset includes the file contents in hex format, lots of work would have been allocated to preprocessing the files. Also the dataset is quite popular in the cybersecurity research community with over 50 research papers and thesis works citing the dataset, this would violate the Capstone rules.
The models are not very refined and some tuning might increase the Accuracy (or F1 Score for the matter).
Some model approaches were not tested like: 
* Neuronal Network
* Gradient Descent

# System
## Hardware
All above computations are done with an `r {get_cpu().model_name}` CPU with `r {get_cpu().no_of_cores}` and `r {get_ram()}`` of RAM.
```{r}
#get_cpu()
#get_ram()
```

## Software
This report is compiled using R markdown with RStudio.
```{r}
sessionInfo()
```

# Resources
[1] Rafael Irizarry. 2018. Introduction to Data Science. <https://rafalab.dfci.harvard.edu/dsbook/>
[2] Malware static and dynamic features VxHeaven and Virus Total Data Set <https://archive.ics.uci.edu/ml/datasets/Malware+static+and+dynamic+features+VxHeaven+and+Virus+Total#>
[3] PE Format <https://learn.microsoft.com/en-us/windows/win32/debug/pe-format>
[4] cuckoo sandbox <https://cuckoosandbox.org/>
