##########################################################
#      EDX HarvardX PH125.9x DataScience Capstone 
##########################################################

# Some includes are not necessary for this R script (e.g. rmarkdown) but for 
# consistency with the report they are included anyway
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(devtools)) install.packages("devtools")
if(!require(benchmarkme)) install.packages("benchmarkme")
if(!require(rmarkdown)) install.packages("rmarkdown")
if(!require(lubridate)) install.packages("lubridate")
if(!require(scales)) install.packages("scales")
if(!require(parallel)) install.packages("parallel")
if(!require(stringr)) install.packages("stringr")
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr")
if(!require(httr)) install.packages("httr")
if(!require(e1071)) install.packages("e1071")
if(!require(class)) install.packages("class")
if(!require(ranger)) install.packages("ranger")
if(!require(rpart)) install.packages("rpart")
if(!require(xgboost)) install.packages("xgboost")
if(!require(glmnet)) install.packages("glmnet")
if(!require(GGally)) install.packages("GGally")
if(!require(Rtsne)) install.packages("Rtsne")
if(!require(rgl)) install.packages("rgl")
if(!require(reshape2)) install.packages("reshape2")
if(!require(plyr)) install.packages("plyr")
if(!require(scales)) install.packages("scales")

library(ggplot2)
library(benchmarkme)
library(rmarkdown)
library(lubridate)
library(scales)
library(parallel)
library(stringr)
library(kableExtra)
library(tidyverse)
library(caret)
library(readr)
library(httr)
library(e1071)
library(class)
library(ranger)
library(rpart)
library(xgboost)
library(glmnet)
library(GGally)
library(Rtsne)
library(rgl)
library(reshape2) 
library(plyr) 
library(scales)

################# DATASET ####################
# download dataset from UCI archive https://archive.ics.uci.edu/ml/datasets/Malware+static+and+dynamic+features+VxHeaven+and+Virus+Total#
download_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00541/datasets_DetectingMalwareUsingAHybridApproach.zip'
download.file(download_url, 'datasets.zip', mode='wb')

# unzip datasets.zip that was downloaded into data
unzip_result <- unzip("datasets.zip", overwrite = TRUE, exdir="data")
# this will unzip several files

list.files(path = "data/", full.names = TRUE)
# Note: for this tho work the Working Directory must be set on the path of this R script
# these are the csv extracted
# - staDynBenignLab.csv: 1086 features extracted from 595 files on MS Windows 7 and 8, obtained Program Files directory.
# - staDynVxHeaven2698Lab.csv: 1087 features extracted from 2698 files of VxHeaven dataset.
# - staDynVt2955Lab.csv: 1087 features extracted from 2955 provided by Virus Total in 2018.

# load these csv
# into benign
benign <- read_csv("data/staDynBenignLab.csv")
# inspect benign
head(benign)
dim(benign)

# into vxheaven
vxheaven <- read_csv("data/staDynVxHeaven2698Lab.csv")
# inspect vxheaven
head(vxheaven)
dim(vxheaven)

# into vt
vt <- read_csv("data/staDynVt2955Lab.csv")
# inspect vt
head(vt)
dim(vt)

# check what columns are the same
# first load all column names from each dataset
cols_benign <- colnames(benign)
cols_vxheaven <- colnames(vxheaven)
cols_vt <- colnames(vt)

# Find common column names
common_cols <- intersect(intersect(cols_benign, cols_vxheaven), cols_vt)
common_cols

# Find different column names
different_cols <- setdiff(union(union(cols_benign, cols_vxheaven), cols_vt), common_cols)
different_cols
# "...1", "filename", "__vbaVarIndexLoad", "SafeArrayPtrOfIndex"

# check for any NA
anyNA(benign)   # none
anyNA(vxheaven) # none
anyNA(vt)       # none

# remove unique columns from each dataset
benign <- benign[, common_cols]
vxheaven <- vxheaven[, common_cols]
vt <- vt[, common_cols]
# now each dataset has 1085 features

# add classes to each dataset
benign$class <- 0     # benign: 0
vxheaven$class <- 1   # vxheaven (static): 1
vt$class <- 2         # vt (dynamic): 2

# create one large dataset 
data <- rbind(benign, vxheaven, vt)
# inspect resulting dataset
dim(data)

################# ANALYSIS ####################
# check variable variance
# Calculate the variance of each variable in the dataset
variances <- apply(data, 2, var)

# Find which variances are equal to 0
zero_variances <- which(variances == 0)
length(zero_variances)
# 840 variables have 0 variance, all values of e.g. "count_file_renamed"
# is always = 0 and has no benefit for the analysis or training.

# Exclude any variables with 0 variance
nonzero_variances <- variances[variances > 0]
length(nonzero_variances)
# only 246 variables remain

# Create a bar plot of the variances
barplot(nonzero_variances, main="Feature Variance", las=2, log="y", xlab="", ylab="variance log10")

# create a dataset with only relevant variables
nonzero_cols <- colnames(data)[variances != 0]
data_v2 <- data[, nonzero_cols]

# set color for each class
class_colors <- c("lightskyblue", "orchid", "violetred")
# and set short class names for complex plots
class_names <- c("ok", "stat", "dyn")

####### Plots with lots of features ########

# What plots to use with lots of features.
# Scatterplot Matrix
# Making a scatterplot matrix of a dataset with over 200 columns can be 
# challenging, as the resulting plot will be very large and difficult to 
# interpret. One approach is to use a subset of the columns, or to group 
# the columns in some meaningful way before creating the plot. 
# Another approach is to use a dimensionality reduction technique such as 
# principal component analysis (PCA) to reduce the number of variables in the plot.
# or SNE

# example of a scatterprott
data_sub <- data_v2

# Perform PCA on the subset of data
mydata_pca <- preProcess(data_sub, method = "pca")
mydata_pca <- predict(mydata_pca, data_sub)

# Create a scatterplot matrix of the first four principal components
ggpairs(data = mydata_pca[, 1:10])

# SNE plot
#Stochastic Neighbor Embedding (SNE) is a nonlinear dimensionality reduction technique 
#that is often used for visualizing high-dimensional datasets in a lower-dimensional space. 
#The basic idea behind SNE is to map the high-dimensional dataset onto a lower-dimensional 
#space in such a way that the pairwise similarities between data points are preserved as much as possible.

#In a SNE plot, each data point is represented by a point in a 2D or 3D space, and the 
#distance between the points in the SNE plot reflects the similarity (or dissimilarity) between 
#the corresponding data points in the original high-dimensional space. Points that are close together 
#in the SNE plot are similar to each other, while points that are far apart are dissimilar.

#SNE works by modeling the similarities between data points in the high-dimensional space as 
#probabilities, and then minimizing the difference between these probabilities and the probabilities 
#of the corresponding points in the lower-dimensional space. The result is a mapping that preserves the 
#pairwise similarities between the data points as well as possible.

#SNE can be a powerful tool for exploring and visualizing high-dimensional datasets, as it can reveal 
#underlying patterns and structures that might be difficult to discern in the original space. However, 
#it is important to keep in mind that the interpretation of the SNE plot may be limited by the inherent 
#limitations of the algorithm and the nature of the data being analyzed.

sne <- Rtsne(data_v2, is_distance = FALSE, perplexity=100, verbose = TRUE)
plot(sne$Y, pch = 16, main = "SNE plot")


###### Datatypes #######

# list datatypes + class
types <- c('UINT', 'LONG', 'BOOL', 'WORD','byte','word','char','int','class')

# subset of these types 
types_subset <- subset(data_v2, select=types)

# give the classes some better names
types_subset$class <- factor(types_subset$class, levels = c(0, 1, 2), labels = class_names)

# boxplot for the type "int"
ggplot(types_subset, aes(x = class, y = int, fill=factor(class))) +
  geom_boxplot(alpha = 0.8) + 
  scale_fill_manual(values = class_colors, 
                    labels = class_names) +
  scale_y_log10(limits = c(1, max(types_subset$int))) +
  labs(x = "Class", y = "log(int)") +
  theme_light()

# boxplot for type "char"
ggplot(types_subset, aes(x = class, y = char, fill=factor(class))) +
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = class_colors, 
                    labels = class_names) +
  scale_y_log10(limits = c(1, max(types_subset$char))) +
  labs(x = "Class", y = "log(char)") +
  theme_light()

# melt the subset for types for following boxplot 
types_subset.m <- melt(types_subset, id.var = "class")

# boxplot for all datatypes
ggplot(data = types_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = class_colors, 
                    labels = class_names) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA)) +
  theme_light()

# scatterplot matrix of datatypes
ggpairs(types_subset, columns=1:8, ggplot2::aes(colour=class, alpha=0.3)) + 
  scale_color_manual(values=class_colors) +
  theme_light()


###### File and executable #######

# split available file features into two datasets because of size of plots
# column with file characteristics
file_1 <- c('filesize','number_of_rva_and_sizes','size_code','SizeOfHeaders','section_alignment','file_alignment','size_of_headers','subsystem','dll_characteristics','AddressOfEntryPoint','class')

# create a second one for two smaller plots
file_2 <- c('size_of_stack_reserve','size_of_stack_commit','size_of_heap_reserve','size_of_heap_commit','image_base','Size_image','BaseOfCode','number_of_rva_and_sizes.1','number_of_IAT_entires.1','files_operations','count_file_opened','count_dll_loaded','class')

# subset of these types 
file_1_subset <- subset(data_v2, select=file_1)
file_2_subset <- subset(data_v2, select=file_2)

# give the classes some better names
file_1_subset$class <- factor(file_1_subset$class, levels = c(0, 1, 2), labels = class_names)
file_2_subset$class <- factor(file_2_subset$class, levels = c(0, 1, 2), labels = class_names)

# melt the subset for types for following boxplot 
file_1_subset.m <- melt(file_1_subset, id.var = "class")
file_2_subset.m <- melt(file_2_subset, id.var = "class")

# array of boxplot for first file subset
ggplot(data = file_1_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = class_colors, 
                    labels = class_names) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA))

# matrix scatterplot of the first file subset
ggpairs(file_1_subset, columns=1:8, ggplot2::aes(colour=class, alpha=0.3)) +
  scale_color_manual(values=class_colors) +
  theme_light()

# array of boxplot for second file subset
ggplot(data = file_2_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = class_colors, 
                    labels = class_names) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA))

# matrix scatterplot of the second file subset
ggpairs(file_2_subset, columns=1:8, ggplot2::aes(colour=class, alpha=0.3)) +
  scale_color_manual(values=class_colors) +
  theme_light()


# also add a third subset
file3 <- c('loader_flags','count_mutex','count_file_read','count_file_written','count_file_exists','count_file_deleted','count_file_copied','count_regkey_written','count_regkey_deleted','class')

# subset of these types 
file3_subset <- subset(data_v2, select=file3)

# give the classes some better names
file3_subset$class <- factor(file3_subset$class, levels = c(0, 1, 2), labels = class_names)

# melt the subset for types for following boxplot 
file3_subset.m <- melt(file3_subset, id.var = "class")

# boxplot for the third subset
ggplot(data = file3_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = c("orchid", "violetred"), 
                    labels = c("Malware static", "Malware dynamic")) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA))

# matrix scatterplot of the third file subset
ggpairs(file3_subset, columns=1:8, ggplot2::aes(colour=class, alpha=0.3)) +
  scale_color_manual(values=class_colors) +
  theme_light()


###### Compiler/Linker #######
# compiler and linker features subset list
compiler <- c('characteristics','magic','size_init_data','size_uninit_data','compile_date','major_linker_version','minor_linker_version','class')

# subset of these comiler/linker statistics 
compiler_subset <- subset(data_v2, select=compiler)

# give the classes some better names
compiler_subset$class <- factor(compiler_subset$class, levels = c(0, 1, 2), labels = class_names)

# melt the subset for types for following boxplot 
compiler_subset.m <- melt(compiler_subset, id.var = "class")

# matrix plot of compiler and linker features
ggpairs(compiler_subset, columns=1:8, ggplot2::aes(colour=class, alpha=0.3)) + 
  scale_color_manual(values=class_colors) +
  theme_light()

# rescale everything into 0.0 to 1.0
compiler_subset.m <- ddply(compiler_subset.m, .(variable), transform, rescale=rescale(value))

# heatmap plot
ggplot(compiler_subset.m, aes(variable, class)) +
  geom_tile(aes(fill = rescale), colour = "grey") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

###### OS #######
# list of OS versions
os <- c('major_operating_system_version','minor_operating_system_version','major_image_version','minor_image_version','major_subsystem_version','minor_subsystem_version','class')

# subset of these comiler/linker statistics 
os_subset <- subset(data_v2, select=os)

# give the classes some better names
os_subset$class <- factor(os_subset$class, levels = c(0, 1, 2), labels = class_names)

# matrix plot of OS version features
ggpairs(os_subset, columns=1:6, ggplot2::aes(colour=class, alpha=0.3)) + 
  scale_color_manual(values=class_colors) +
  theme_light()


###### ASM #######
# list of ASM functions
asm_1 <- c('dd','db','dw','arg','add','cli','cmc','cmp','imul','inc','int.1','jb','je','jg','jge','jl','class')
asm_2 <- c('jmp','jno','jo','lea','loope','mov','mul','neg','not','or','sub','test','xchg','xor','nop','class')

# subset of these comiler/linker statistics 
asm_1_subset <- subset(data_v2, select=asm_1)
asm_2_subset <- subset(data_v2, select=asm_2)

# give the classes some better names
asm_1_subset$class <- factor(asm_1_subset$class, levels = c(0, 1, 2), labels = class_names)
asm_2_subset$class <- factor(asm_2_subset$class, levels = c(0, 1, 2), labels = class_names)

# melt the subset for ASM for following boxplot 
asm_1_subset.m <- melt(asm_1_subset, id.var = "class")
asm_2_subset.m <- melt(asm_2_subset, id.var = "class")

# boxplots of the first ASM subset
ggplot(data = asm_1_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = class_colors, 
                    labels = class_names) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA)) + 
  theme_light()

# boxplots of the second ASM subset
ggplot(data = asm_2_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = class_colors, 
                    labels = class_names) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA)) + 
  theme_light()


###### DLL #######
# list all dll features
dlls <- c('dll','dll_characteristics','count_dll_loaded','class')

# subset of these comiler/linker statistics 
dll_subset <- subset(data_v2, select=dlls)

# give the classes some better names
dll_subset$class <- factor(dll_subset$class, levels = c(0, 1, 2), labels = class_names)

# melt together
dll_subset.m <- melt(dll_subset, id.var = "class")

# boxplot of dll, dll_characteristics and count_dll_loaded
ggplot(data = dll_subset.m, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=class)) + 
  scale_fill_manual(values = class_colors, 
                    labels = class_names) +
  facet_wrap( ~ variable, scales="free") +
  scale_y_log10(limits = c(NA, NA)) + 
  theme_light()

# matrix plot of these DLL features
ggpairs(dll_subset, columns=1:3, ggplot2::aes(colour=class, alpha=0.3)) + 
  scale_color_manual(values=class_colors) +
  theme_light()

################# MODEL ####################

set.seed(42) # because 42 is always the answer

# split data into training (80%) and test (20%) set
test_index <- createDataPartition(y=data_v2$class, times=1, p=0.2, list=FALSE)
colnames(data_v2) <- make.names(colnames(data_v2))

# fill test and train set of cleaned dataset
train_set <- data_v2[-test_index,]
test_set <- data_v2[test_index,]

# prepare the results table
ml_results <- tibble()

############################################
# Guessing
# make "prediction" on the test set with the "guess model" by apply randomly 0,1 or 2
guess_model <- factor(sample(c(0, 1, 2), length(test_set$class), replace=TRUE), levels=c(0,1,2))

# Create the confusion matrix
guess_cm <- confusionMatrix(table(factor(test_set$class), guess_model))

# Get overall accuracy and F1 scores
guess_acc <- guess_cm$overall['Accuracy']
guess_f1_c0 <- guess_cm$byClass[, 'F1'][1]
guess_f1_c1 <- guess_cm$byClass[, 'F1'][2]
guess_f1_c2 <- guess_cm$byClass[, 'F1'][3]

# add guessing results to table
ml_results <- ml_results %>%
  bind_rows(tibble(Model="Guessing", Accuracy=guess_acc, "F1 Class0"=guess_f1_c0, "F1 Class1"=guess_f1_c1, "F1 Class2"=guess_f1_c2))
# Accuracy 0.3336
# around 33% was expected

############################################
# Naive Bayes

# Train the Naive Bayes model
nb_model <- naiveBayes(class ~ ., data=train_set)

# Make predictions on test set
nb_pred <- predict(nb_model, newdata=test_set)

# creater confusion matrix
nb_cm <- confusionMatrix(table(factor(test_set$class), nb_pred))

# Get overall accuracy and F1 scores
nb_acc <- nb_cm$overall['Accuracy']
nb_f1_c0 <- nb_cm$byClass[, 'F1'][1]
nb_f1_c1 <- nb_cm$byClass[, 'F1'][2]
nb_f1_c2 <- nb_cm$byClass[, 'F1'][3]

# add results to table
ml_results <- ml_results %>%
  bind_rows(tibble(Model="Naive Bayes", Accuracy=nb_acc, "F1 Class0"=nb_f1_c0, "F1 Class1"=nb_f1_c1, "F1 Class2"=nb_f1_c2))
# Accuracy 0.2624 

############################################
# SVM (1min)
# Train SVM model
svm_model <- svm(class ~ ., data=train_set)

# Make predictions on test set
svm_pred <- factor(round(predict(svm_model, newdata=test_set)), levels=c(0,1,2))

# creater confusion matrix
svm_cm <- confusionMatrix(table(factor(test_set$class), svm_pred))

# Get overall accuracy and F1 scores
svm_acc <- svm_cm$overall['Accuracy']
svm_f1_c0 <- svm_cm$byClass[, 'F1'][1]
svm_f1_c1 <- svm_cm$byClass[, 'F1'][2]
svm_f1_c2 <- svm_cm$byClass[, 'F1'][3]

# add results to table
ml_results <- ml_results %>%
  bind_rows(tibble(Model="SVM", Accuracy=svm_acc, "F1 Class0"=svm_f1_c0, "F1 Class1"=svm_f1_c1, "F1 Class2"=svm_f1_c2))
# Accuracy 0.5204163 

############################################
# KNN (3min)
# Train a KNN model and make predictions on test set
knn_model <- knn(train_set[, -ncol(train_set)], test_set[, -ncol(test_set)], train_set$class, k = 3)

# create confusion matrix
knn_cm <- confusionMatrix(table(factor(test_set$class), knn_model))

# Get overall accuracy and F1 scores
knn_acc <- knn_cm$overall['Accuracy']
knn_f1_c0 <- knn_cm$byClass[, 'F1'][1]
knn_f1_c1 <- knn_cm$byClass[, 'F1'][2]
knn_f1_c2 <- knn_cm$byClass[, 'F1'][3]

# add results to table
ml_results <- ml_results %>%
  bind_rows(tibble(Model="KNN", Accuracy=knn_acc, "F1 Class0"=knn_f1_c0, "F1 Class1"=knn_f1_c1, "F1 Class2"=knn_f1_c2))
# Accuracy 0.9104 

############################################
# Decision Trees

# Train a Decision Tree model
dt_model <- rpart(class ~ ., data=train_set)

# Make predictions on test set
dt_pred <- round(predict(dt_model, newdata=test_set))

# create confusion matrix
dt_cm <- confusionMatrix(table(factor(test_set$class), dt_pred))

# Get overall accuracy and F1 scores
dt_acc <- dt_cm$overall['Accuracy']
dt_f1_c0 <- dt_cm$byClass[, 'F1'][1]
dt_f1_c1 <- dt_cm$byClass[, 'F1'][2]
dt_f1_c2 <- dt_cm$byClass[, 'F1'][3]

# add results to table
ml_results <- ml_results %>%
  bind_rows(tibble(Model="Decision Tree", Accuracy=dt_acc, "F1 Class0"=dt_f1_c0, "F1 Class1"=dt_f1_c1, "F1 Class2"=dt_f1_c2))
# Accuracy 0.9088 

############################################
# Random Forest

# Train a Random Forest model
rf_model <- ranger(class ~ ., data=train_set, num.trees=480)

# Make predictions on test set
rf_pred <- factor(round(predict(rf_model, data = test_set)$predictions), levels = c(0, 1, 2))

# Calculate confusion matrix
rf_cm <- confusionMatrix(table(factor(test_set$class), rf_pred))

# Get overall accuracy and F1 scores
rf_acc <- rf_cm$overall['Accuracy']
rf_f1_c0 <- rf_cm$byClass[, 'F1'][1]
rf_f1_c1 <- rf_cm$byClass[, 'F1'][2]
rf_f1_c2 <- rf_cm$byClass[, 'F1'][3]

# add results to table
ml_results <- ml_results %>%
  bind_rows(tibble(Model="Random Forest", Accuracy=rf_acc, "F1 Class0"=rf_f1_c0, "F1 Class1"=rf_f1_c1, "F1 Class2"=rf_f1_c2))
# Accuracy 0.9528 

############################################
# Gradient Boosting 

# Train a Gradient Boosting model
gb_model <- xgboost(data = as.matrix(train_set[, -1]),
                    label = train_set$class,
                    nrounds = 3,
                    objective = "multi:softmax",
                    num_class = 3,
                    eval_metric = "merror",
                    verbose = 0)

# Make predictions on test set
# convert predictions to integer class labels
gb_pred <- factor(predict(gb_model, as.matrix(test_set[, -1])), levels=c(0,1,2))

# Confusion matrix
gb_cm <- confusionMatrix(table(factor(test_set$class), gb_pred))

# Get overall accuracy and F1 scores
gb_acc <- gb_cm$overall['Accuracy']
gb_f1_c0 <- gb_cm$byClass[, 'F1'][1]
gb_f1_c1 <- gb_cm$byClass[, 'F1'][2]
gb_f1_c2 <- gb_cm$byClass[, 'F1'][3]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Gradient Boost", Accuracy=gb_acc, "F1 Class0"=gb_f1_c0, "F1 Class1"=gb_f1_c1, "F1 Class2"=gb_f1_c2))
# Accuracy 1

# System Infos:
## Hardware
hw_cpu <- get_cpu()
hw_cpu$model_name
hw_cpu$no_of_cores
sessionInfo()
