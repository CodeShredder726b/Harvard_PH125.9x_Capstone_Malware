##########################################################
#      EDX HarvardX PH125.9x DataScience Capstone 
##########################################################
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(devtools)) install.packages("devtools")
if(!require(benchmarkme)) install.packages("benchmarkme")
if(!require(rmarkdown)) install.packages("rmarkdown")
if(!require(lubridate)) install.packages("lubridate")
if(!require(scales)) install.packages("scales")
if(!require(parallel)) install.packages("parallel")
if(!require(stringr)) install.packages("stringr")
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr")
if(!require(httr)) install.packages("httr")
if(!require(e1071)) install.packages("e1071")
if(!require(class)) install.packages("class")
if(!require(ranger)) install.packages("ranger")
if(!require(rpart)) install.packages("rpart")
if(!require(xgboost)) install.packages("xgboost")
if(!require(nnet)) install.packages("nnet")
if(!require(glmnet)) install.packages("glmnet")
if(!require(GGally)) install.packages("GGally")

devtools::install_github("ldurazo/kaggler")

library(ggplot2)
library(benchmarkme)
library(rmarkdown)
library(lubridate)
library(scales)
library(parallel)
library(stringr)
library(kableExtra)
library(tidyverse)
library(caret)
library(readr)
library(httr)
library(kaggler)
library(e1071)
library(class)
library(ranger)
library(rpart)
library(xgboost)
library(nnet)
library(glmnet)
library(GGally)

################# DATASET ####################
# download dataset from UCI archive https://archive.ics.uci.edu/ml/datasets/Malware+static+and+dynamic+features+VxHeaven+and+Virus+Total#
download_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00541/datasets_DetectingMalwareUsingAHybridApproach.zip'
download.file(download_url, 'datasets.zip', mode='wb')

# unzip
unzip_result <- unzip("datasets.zip", overwrite = TRUE)

# this will unzip several files/folders
list.files()
# these are the csv extracted
# - staDynBenignLab.csv: 1086 features extracted from 595 files on MS Windows 7 and 8, obtained Program Files directory.
# - staDynVxHeaven2698Lab.csv: 1087 features extracted from 2698 files of VxHeaven dataset.
# - staDynVt2955Lab.csv: 1087 features extracted from 2955 provided by Virus Total in 2018.

# load these csv
benign <- read_csv("staDynBenignLab.csv")
head(benign)
dim(benign)

vxheaven <- read_csv("staDynVxHeaven2698Lab.csv")
head(vxheaven)
dim(vxheaven)

vt <- read_csv("staDynVt2955Lab.csv")
head(vt)
dim(vt)

# check what columns are the same
cols_benign <- colnames(benign)
cols_vxheaven <- colnames(vxheaven)
cols_vt <- colnames(vt)

# Find common column names
common_cols <- intersect(intersect(cols_benign, cols_vxheaven), cols_vt)
common_cols

# Find different column names
different_cols <- setdiff(union(union(cols_benign, cols_vxheaven), cols_vt), common_cols)
different_cols
# "...1", "filename", "__vbaVarIndexLoad", "SafeArrayPtrOfIndex"

# check for any NA
anyNA(benign)
anyNA(vxheaven)
anyNA(vt)
# none

# remove unique columns
benign <- benign[, common_cols]
vxheaven <- vxheaven[, common_cols]
vt <- vt[, common_cols]
# now each dataset has 1085 features

# classify each dataset
benign$class <- 0
vxheaven$class <- 1
vt$class <- 2

# create one large dataset 
data <- rbind(benign, vxheaven, vt)
dim(data)

################# ANALYSIS ####################
# check variable variance
# Calculate the variance of each variable in the dataset
variances <- apply(data, 2, var)

# Find which variances are equal to 0
zero_variances <- which(variances == 0)
length(zero_variances)
# 840 variables have 0 variance, all values of e.g. "count_file_renamed"
# is always = 0 and has no benefit for the analysis or training.

# Exclude any variables with 0 variance
nonzero_variances <- variances[variances > 0]
# only 246 variables remain

# Create a bar plot of the variances
barplot(nonzero_variances, main="Log Variance", las=2, log="y", xlab="var", ylab="variance")

## Calculate standard deviations
#sd_values <- sqrt(nonzero_variances)
#
#barplot(nonzero_variances, main="Log Variance", las=2, log="y", xlab="var", ylab="variance")
#
## Add error bars
#arrows(x0 = 1:length(nonzero_variances), y0 = nonzero_variances - sd_values, 
#       x1 = 1:length(nonzero_variances), y1 = nonzero_variances + sd_values, 
#       angle=90, code=3, length=0.05)

# create a dataset with only relevant variables
nonzero_cols <- colnames(data)[variances != 0]
data_v2 <- data[, nonzero_cols]

# Scatterplot Matrix
#Making a scatterplot matrix of a dataset with over 200 columns can be 
#challenging, as the resulting plot will be very large and difficult to 
#interpret. One approach is to use a subset of the columns, or to group 
#the columns in some meaningful way before creating the plot. 
#Another approach is to use a dimensionality reduction technique such as 
#principal component analysis (PCA) to reduce the number of variables in the plot.
# or SNE
data_sub <- data

# Perform PCA on the subset of data
mydata_pca <- preProcess(data_sub, method = "pca")
mydata_pca <- predict(mydata_pca, data_sub)

# Create a scatterplot matrix of the first four principal components
ggpairs(data = mydata_pca[, 1:4])


# Heatmap

# Parallel Coordinates Plot

# Boxplots

# Correlation Matrix

# Principal Components Analysis


#############
c('ent_whole_file','ent_mean','ent_var','ent_median','ent_max','ent_min	max_min')

# File/exe
c('.rdata:','.data:','.text:','filesize','number_of_sections','number_of_IAT_entries','number_of_rva_and_sizes','size_code','SizeOfHeaders','section_alignment','file_alignment')
c('size_of_headers','subsystem','dll_characteristics','loader_flags',,'AddressOfEntryPoint','SizeOfHeaders.1	CheckSum','size_of_stack_reserve','size_of_stack_commit','size_of_heap_reserve','size_of_heap_commit','image_base','Size_image','BaseOfCode','number_of_rva_and_sizes.1','number_of_IAT_entires.1','count_mutex','files_operations','count_file_read','count_file_written')
c('count_file_exists','count_file_deleted','count_file_copied	count_file_renamed','count_regkey_written','count_regkey_deleted','count_file_opened','count_dll_loaded')

# Compiler/Linker
c('compile_date','characteristics','magic','major_linker','major_linker_version','minor_linker_version','size_init_data','size_uninit_data','major_operating_system_version','minor_operating_system_version','major_image_version','minor_image_version','major_subsystem_version','minor_subsystem_version')

# Imports
c('number_of_imports.1','Import','Imports','__IMPORT_','number_of_import_symbols','number_of_imports','imported_symbols','datadir_IMAGE_DIRECTORY_ENTRY_IMPORT_size')

# Datatypes
c('UINT', 'LONG', 'BOOL', 'WORD', 'BYTES','byte','word','dword','char','DWORD','void','int','')

# ASM/Functions
c('dd','db','dw','stdcall','arg','edx','esi','es','fs','ds','ss','gs','cs','ah','al','ax','bh','bl','bx','ch','cl','cx','dh')
c('dl','dx','eax','ebp','ebx','ecx','edi','esp','add','al.1','bt','call','cdq')
c('cld','cli','cmc','cmp','const','cwd','daa','db.1','dd.1','dec','dw.1','enp','ends')
c('faddp','fchs','fdiv','fdivp','fdivr','fild','fistp','fld','fstcw','fstcwimul','fstp','fword','fxch')
c('imul','in','inc','ins','int.1','jb','je','jg','jge','jl','jmp','jnb','jno','jnz')
c('jo','jz','lea','loope','mov','movzx','mul','near','neg','not','or','out','outs','pop','popf','proc.1','push','pushf','rcl')
c('rcr','rdtsc','rep','ret','retn','rol','ror','sal','sar','sbb','scas','setb','setle','setnle','setnz','setz','shl','shld','shr','	sidt','stc','std','sti','stos','sub','test','wait','xchg','xor','nop','wcslen')

# GUI / Menu

# Events

# System
c('System32','hkey_current_user','hkey_local_machine','major_operating_system_version','minor_operating_system_version','major_image_version','minor_image_version','major_subsystem_version','minor_subsystem_version')

# DLL
c('DllEntryPoint','dll','DLL','imported_dll_freq','dll_characteristics','count_dll_loaded')

################# MODEL ####################

set.seed(42) # because 42 is always the answer

# split data into training and test set
test_index <- createDataPartition(y = data$class, times=1, p=0.3, list=FALSE)
colnames(data) <- make.names(colnames(data))

train_set <- data[-test_index,]
test_set <- data[test_index,]

ml_results <- tibble()

############################################
# Guessing
guess_model <- factor(sample(c(0, 1, 2), length(test_set$class), replace=TRUE), levels=c(0,1,2))
guess_acc <- sum(guess_model == test_set$class) / nrow(test_set)
# 0.3397333
# around 33% was expected

# Calculate F1 score
guess_cm <- confusionMatrix(table(factor(test_set$class), guess_model))
guess_f1 <- guess_cm$byClass[4]
# 0.8995327

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Guessing", Accuracy=guess_acc, F1=guess_f1))

############################################
# SVM (1min)
svm_model <- svm(class ~ ., data=train_set)
svm_pred <- factor(round(predict(svm_model, newdata=test_set)), levels=c(0,1,2))
svm_acc <- sum(svm_pred == test_set$class) / nrow(test_set)
# 0.5173333

svm_cm <- confusionMatrix(table(factor(test_set$class), svm_pred))
svm_f1 <- svm_cm$byClass[4]
# 0.9061333

ml_results <- ml_results %>%
  bind_rows(tibble(Model="SVM", Accuracy=svm_acc, F1=svm_f1))

############################################
# KNN (3min)
knn_model <- knn(train_set[, -ncol(train_set)], test_set[, -ncol(test_set)], train_set$class, k = 3)
knn_acc <- sum(knn_model == test_set$class) / nrow(test_set)
# 0.9098667

# Calculate F1 score
knn_cm <- confusionMatrix(table(factor(test_set$class), knn_model))
knn_f1 <- knn_cm$byClass[4]
# 0.9836353

ml_results <- ml_results %>%
  bind_rows(tibble(Model="KNN", Accuracy=knn_acc, F1=knn_f1))

############################################
# Random Forest
rf_model <- ranger(class ~ ., data=train_set, num.trees=100)
rf_pred <- factor(round(predict(rf_model, data = test_set)$predictions), levels = c(0, 1, 2))
rf_acc <- sum(rf_pred == test_set$class) / nrow(test_set)
# 0.9349333

# Calculate F1 score
rf_cm <- confusionMatrix(table(factor(test_set$class), rf_pred))
rf_f1 <- rf_cm$byClass[4]
# 0.9832176

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Random Forest", Accuracy=rf_acc, F1=rf_f1))

############################################
# Naive Bayes
nb_model <- naiveBayes(class ~ ., data=train_set)
nb_pred <- predict(nb_model, newdata=test_set)
nb_acc <- sum(nb_pred == test_set$class) / nrow(test_set)
# 0.2330667

# Calculate F1 score
nb_cm <- confusionMatrix(table(factor(test_set$class), nb_pred))
nb_f1 <- nb_cm$byClass[4]
# 0.8977273

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Naive Bayes", Accuracy=nb_acc, F1=nb_f1))

############################################
# Decision Trees
dt_model <- rpart(class ~ ., data=train_set)
dt_pred <- round(predict(dt_model, newdata=test_set))
dt_acc <- sum(dt_pred == test_set$class) / nrow(test_set)
# 0.9226667

# Calculate F1 score
dt_cm <- confusionMatrix(table(factor(test_set$class), dt_pred))
dt_f1 <- dt_cm$byClass[4]
# 0.8977273

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Decision Trees", Accuracy=dt_acc, F1=dt_f1))

############################################
# Gradient Descent
gd_model <- glmnet(train_set, train_set$class, family = "multinomial")
#gd_model <- glmnet(train_set, train_set$class, family = "multinomial", alpha = 1, lambda = 0.01, standardize = TRUE, intercept = TRUE)
gd_pred <- predict(gd_model, newx = test_set, type="class")
#gd_acc <- sum(gd_pred == test_set$class) / nrow(test_set)
# 
#
# Calculate F1 score
#gd_cm <- confusionMatrix(table(factor(test_set$class), gd_pred))
#gd_f1 <- gd_cm$byClass[4]
# 
#
#ml_results <- ml_results %>%
#  bind_rows(tibble(Model="Gradient Descent", Accuracy=gd_acc, F1=gd_f1))

############################################
# Gradient Boosting 
gb_model <- xgboost(data = as.matrix(train_set[, -1]),
                    label = train_set$class,
                    nrounds = 100,
                    objective = "multi:softmax",
                    num_class = 3,
                    eval_metric = "merror")

gb_pred <- factor(predict(gb_model, as.matrix(test_set[, -1])), levels=c(0,1,2))
# convert predictions to integer class labels
gb_acc <- sum(gb_pred == test_set$class) / nrow(test_set)
gb_cm <- confusionMatrix(table(factor(test_set$class), gb_pred))

#gb_cm <- confusionMatrix(table(test_set$class, gb_pred)
gb_f1 <- gb_cm$byClass[4]

ml_results <- ml_results %>%
  bind_rows(tibble(Model="Gradient Boosting", Accuracy=gb_acc, F1=gb_f1))

############################################
# Neuronal Network
#nn_model <- nnet(class ~ ., data = train_set, size = 10, maxit = 1000)
#nn_pred <- predict(nn_model, newdata = test_set, type = "class")
#
#nn_cm <- confusionMatrix(test_set$class, nn_pred)
#nn_f1 <- nn_cm$byClass[4]
#
#ml_results <- ml_results %>%
#  bind_rows(tibble(Model="Neuronal Network", Accuracy=nn_acc, F1=nn_f1))

# System Infos:
## Hardware
hw_cpu <- get_cpu()
hw_cpu$model_name
hw_cpu$no_of_cores
